# åŸºäº Transformer çš„æœºå™¨ç¿»è¯‘ï¼ˆè‹±è¯‘ä¸­ï¼‰


## ğŸ“‹ é¡¹ç›®èƒŒæ™¯


### 1. æœºå™¨ç¿»è¯‘çš„æŠ€æœ¯æ¼”è¿›
æœºå™¨ç¿»è¯‘ (Machine Translation, MT) ä½œä¸ºè‡ªç„¶è¯­è¨€å¤„ç† (NLP) é¢†åŸŸçš„æ ¸å¿ƒä»»åŠ¡ä¹‹ä¸€, ç»å†äº†ä¸‰ä¸ªä¸»è¦å‘å±•é˜¶æ®µ: 
- **è§„åˆ™é©±åŠ¨æ—¶ä»£** (1950s-1990s): åŸºäºè¯­è¨€å­¦ä¸“å®¶åˆ¶å®šçš„è¯­æ³•è§„åˆ™å’ŒåŒè¯­è¯å…¸è¿›è¡Œç›´è¯‘, å—é™äºè¯­è¨€å¤æ‚æ€§éš¾ä»¥å®ç°æµç•…ç¿»è¯‘
- **ç»Ÿè®¡å­¦ä¹ æ—¶ä»£** (2000s-2010s): IBM æå‡ºçš„åŸºäºçŸ­è¯­çš„ç»Ÿè®¡æœºå™¨ç¿»è¯‘ (SMT) æˆä¸ºä¸»æµ, åˆ©ç”¨å¤§è§„æ¨¡åŒè¯­è¯­æ–™åº“å­¦ä¹ ç¿»è¯‘æ¦‚ç‡æ¨¡å‹
- **ç¥ç»ç½‘ç»œæ—¶ä»£** (2017-è‡³ä»Š): 2017 å¹´ Google æå‡ºçš„ Transformer æ¶æ„å¼•å‘é©å‘½, å…¶è‡ªæ³¨æ„åŠ›æœºåˆ¶çªç ´äº†ä¼ ç»Ÿ RNN çš„åºåˆ—å»ºæ¨¡ç“¶é¢ˆ

### 2. è‹±è¯‘ä¸­ä»»åŠ¡çš„ç‰¹æ®ŠæŒ‘æˆ˜
ä¸­æ–‡ä¸è‹±è¯­çš„è·¨è¯­ç§ç¿»è¯‘å­˜åœ¨å¤šé‡éš¾ç‚¹: 
- **ç»“æ„å·®å¼‚**: è‹±è¯­çš„ SVOC (ä¸»è°“å®¾è¡¥) ç»“æ„ä¸ä¸­æ–‡çš„æ„åˆè¯­æ³•å­˜åœ¨æ˜ å°„é¸¿æ²Ÿ
- **è¯­ä¹‰é¸¿æ²Ÿ**: æˆè¯­ (å¦‚ "ç”»è›‡æ·»è¶³")ã€æ–‡åŒ–ä¸“æœ‰é¡¹ (å¦‚ "çº¢åŒ…") çš„ç­‰æ•ˆè¡¨è¾¾é—®é¢˜
- **æ•°æ®ç¨€ç¼ºæ€§**: é«˜è´¨é‡è‹±ä¸­å¹³è¡Œè¯­æ–™è§„æ¨¡ä»…ä¸ºè‹±æ³•åŒè¯­æ•°æ®çš„ 1/5 (WMT 2020 ç»Ÿè®¡)

### 3. Transformer çš„æŠ€æœ¯ä¼˜åŠ¿
æœ¬é¡¹ç›®é€‰ç”¨ Transformer æ¶æ„çš„æ ¸å¿ƒç†ç”±: 
| ç‰¹æ€§                | RNN/LSTM          | Transformer       |
|---------------------|-------------------|-------------------|
| é•¿è·ç¦»ä¾èµ–å»ºæ¨¡       | éšè·ç¦»è¡°å‡         | å…¨å±€æ³¨æ„åŠ›        |
| è®­ç»ƒå¹¶è¡Œåº¦           | åºåˆ—é€æ­¥è®¡ç®—       | å…¨åºåˆ—å¹¶è¡Œ        |
| è®¡ç®—å¤æ‚åº¦           | O(n)              | O(nÂ²)            |
| ä½ç½®æ•æ„Ÿæ€§           | å›ºæœ‰é¡ºåº          | éœ€ä½ç½®ç¼–ç         |


## ğŸ› ï¸ å®‰è£…

### ç¯å¢ƒé…ç½®

```bash
conda create -n translator python=3.10
conda activate translator
pip install -r requirements.txt
```

### æ•°æ®é›†é…ç½®

æœ¬é¡¹ç›®ä½¿ç”¨ [cmn-eng-simple](https://box.nju.edu.cn/d/b8245873f1e44c9fab65/) æ•°æ®é›†, åŒ…å«è‹±ä¸­å¹³è¡Œè¯­æ–™. é€šè¿‡é“¾æ¥ä¸‹è½½å, å°†å…¶æ”¾ç½®åœ¨ `data` ç›®å½•, ç»“æ„å¦‚ä¸‹: 

```
data/
â””â”€â”€ cmn-eng-simple/
    â”œâ”€â”€ training.txt       
    â”œâ”€â”€ validation.txt     
    â”œâ”€â”€ testing.txt       
    â”œâ”€â”€ int2word_cn.json   
    â”œâ”€â”€ word2int_cn.json  
    â”œâ”€â”€ int2word_en.json  
    â””â”€â”€ word2int_en.json  
```

## ğŸš€ å¿«é€Ÿè¿è¡Œ

``` bash
python main.py --period train
python main.py --period eval
```

## ğŸ“Š å®éªŒæŠ¥å‘Š

### æ¨¡å‹æ­å»º

Transformer æ¨¡å‹ç»“æ„å¦‚ä¸‹å›¾æ‰€ç¤º. å…¶ä¸­, ä¸»è¦æ¨¡å—åŒ…æ‹¬å·¦ä¾§çš„ç¼–ç å™¨ï¼ˆEncoderï¼‰, ä»¥åŠå³ä¾§çš„è§£ç å™¨ï¼ˆDecoderï¼‰ä¸¤ä¸ªéƒ¨åˆ†. ç¼–ç å™¨å°†è¾“å…¥åºåˆ—è½¬æ¢ä¸ºä¸Šä¸‹æ–‡å‘é‡, è§£ç å™¨æ ¹æ®ä¸Šä¸‹æ–‡å‘é‡ç”Ÿæˆç›®æ ‡åºåˆ—. 

<img src="figs/transformer.png" height="600">

æˆ‘ä»¬å…ˆä»å°çš„ç»„ä»¶å¼€å§‹å®ç°, æœ€åå†å°†å®ƒä»¬ç»„åˆæˆå®Œæ•´çš„ Transformer æ¨¡å‹. 

1. Input Embedding

    åœ¨å°†è‡ªç„¶è¯­è¨€è¾“å…¥æ¨¡å‹å‰, æˆ‘ä»¬é¦–å…ˆä¼šå¯¹å…¶è¿›è¡Œåˆ†è¯, å†æ ¹æ®è¯æ±‡è¡¨å°†æ¯ä¸ªè¯æ ¹è½¬æ¢ä¸ºå¯¹åº”çš„ token. ä¾‹å¦‚ `i am a student .` ä¼šè¢«è½¬åŒ–ä¸º `[5, 98, 9, 415, 4]`. è¿™æ ·è‡ªç„¶è¯­è¨€å°±å˜æˆäº†è®¡ç®—æœºå¯ä»¥ç†è§£çš„æ•°å€¼å½¢å¼. å½“ç„¶, å¯¹äºæ·±åº¦å­¦ä¹ æ¨¡å‹æ¥è¯´, è¿™è¿˜ä¸å¤Ÿ. ç°åœ¨æ¯ä¸ª token è¿˜æ˜¯å¤„äºæ–‡æœ¬ç©ºé—´å½“ä¸­, æˆ‘ä»¬å¸Œæœ›å°†å…¶æŠ•å½±åˆ°æ¨¡å‹çš„è¯­ä¹‰ç©ºé—´, ä»¥ä¾¿æ¨¡å‹æ›´å¥½åœ°ç†è§£å’Œå¤„ç†å…¶ç‰¹å¾. PyTorch å·²ç»æä¾›äº†ä¸€ä¸ªæ¨¡å— `torch.nn.Embedding` ç”¨äºè¯¥æ“ä½œ. åˆå§‹åŒ–æ—¶, ä¸æœºå™¨ç¿»è¯‘ç›¸å…³çš„å‚æ•°åŒ…æ‹¬

    - è¯æ±‡è¡¨å¤§å°: `num_embeddings (int)`
    - ç‰¹å¾ç»´åº¦: `embedding_dim (int)`
    - å¡«å……æ ‡è®°: `padding_idx (int, optional)`

    åœ¨æ„å»ºæœ€ç»ˆçš„æ¨¡å‹æ—¶, æˆ‘ä»¬ç›´æ¥ä½¿ç”¨å³å¯. 

2. Positional Encoding

    åœ¨è‡ªç„¶è¯­è¨€ä¸­, å•è¯çš„é¡ºåºæ˜¯éå¸¸é‡è¦çš„. ä¸ºäº†è®©æ¨¡å‹èƒ½å¤Ÿç†è§£å•è¯çš„é¡ºåº, æˆ‘ä»¬éœ€è¦ä¸ºæ¯ä¸ªå•è¯æ·»åŠ ä¸€ä¸ªä½ç½®ç¼–ç . ä½ç½®ç¼–ç æ˜¯ä¸€ä¸ªä¸å•è¯åµŒå…¥ï¼ˆword embeddingï¼‰ç›¸åŒç»´åº¦çš„å‘é‡, å®ƒåŒ…å«äº†å•è¯åœ¨å¥å­ä¸­çš„ä½ç½®ä¿¡æ¯. è®ºæ–‡ä¸­ç»™å‡ºçš„ä½ç½®ç¼–ç å…¬å¼å¦‚ä¸‹: 

    $PE_{(pos, 2i)} = sin(pos / 10000^{2i/d_{model}})$

    $PE_{(pos, 2i+1)} = cos(pos / 10000^{2i/d_{model}})$

    å…¶ä¸­, $pos$ æ˜¯å•è¯åœ¨å¥å­ä¸­çš„ä½ç½®, $i$ æ˜¯ä½ç½®ç¼–ç çš„ç»´åº¦ç´¢å¼•, $d_{model}$ æ˜¯åµŒå…¥å‘é‡çš„ç»´åº¦. æˆ‘ä»¬å¯ä»¥ç›´æ¥å°†å…¶ç†è§£ä¸ºä¸€ä¸ªç¼–ç å±‚, Transformer çš„è¾“å…¥é¦–å…ˆé€šè¿‡è¯¥ç¼–ç å±‚, è·å¾—ä½ç½®ç¼–ç . ä»£ç å®ç°å¦‚ä¸‹: 

    ```python
    class PositionEncoding(nn.Module):
        def __init__(self, max_seq_len: int, d_model: int):
            super().__init__()
            assert d_model % 2 == 0, "d_model must be even."
            # ç”±äºä½ç½®ç¼–ç æ˜¯ä¸€ä¸ªäºŒå…ƒå‡½æ•°, å¯ä»¥è€ƒè™‘ä½¿ç”¨ä¸€ä¸ªäºŒç»´çš„çŸ©é˜µæ¥è¡¨ç¤º
            i_pos = torch.linspace(
                0, max_seq_len - 1, max_seq_len
            )  # [0, 1, 2, ..., max_len-1] è¡¨ç¤º pos
            j_dim = torch.linspace(
                0, d_model - 2, d_model // 2
            )  # [0, 2, 4, ..., d_model-2] è¡¨ç¤ºå¶æ•°çš„ dim

            # ç”Ÿæˆä¸€ä¸ª [max_len, d_model//2] çš„ç½‘æ ¼ç½‘æ ¼
            # pos.shape: [max_len, d_model//2], two_i.shape: [max_len, d_model//2]
            pos, two_i = torch.meshgrid(i_pos, j_dim, indexing="ij")
            # pe_two_i.shape: [max_len, d_model//2]
            pe_two_i = torch.sin(pos / (10000 ** (two_i / d_model)))
            # pe_two_i_1.shape: [max_len, d_model//2]
            pe_two_i_1 = torch.cos(pos / (10000 ** (two_i / d_model)))

            # å°† pe_two_i å’Œ pe_two_i_1 æ‹¼æ¥æˆä¸€ä¸ª [max_len, d_model] çš„çŸ©é˜µ
            # è€ƒè™‘å…ˆæ‹¼æˆ [max_len, d_model//2, 2] çš„çŸ©é˜µ, ç„¶åå†å±•å¹³
            # åˆ©ç”¨ torch.stack å°†ä¸¤ä¸ª tensor æ²¿ç€æœ€åä¸€ä¸ªç»´åº¦å †å  (è‹¥ä½¿ç”¨ torch.cat çš„è¯åˆ™ç›´æ¥è¿æ¥äº†, ä¸ç¬¦åˆå¶å¥‡é—´éš”çš„è¦æ±‚)
            pe = torch.stack([pe_two_i, pe_two_i_1], dim=-1)  # [max_len, d_model//2, 2]
            pe = pe.reshape(
                1, max_seq_len, d_model
            )  # [1, max_len, d_model] é¢„ç•™å‡º batch ç»´åº¦
            # æ³¨å†Œä¸º buffer, å³ä¸éœ€è¦æ›´æ–°çš„å‚æ•° (éœ€è¦æ›´æ–°çš„å‚æ•°ä¸º parameter)
            # å½“æˆ‘ä»¬ä½¿ç”¨ model.to(device) æ—¶, buffer å’Œ parameter éƒ½ä¼šè‡ªåŠ¨è½¬ç§»åˆ°å¯¹åº”çš„è®¾å¤‡ä¸Š
            self.register_buffer("pe", pe, False)

        def forward(self, x: torch.Tensor) -> torch.Tensor:
            """
            Args:
                x: [batch_size, seq_len, d_model]
            Returns:
                x: [batch_size, seq_len, d_model]
            """
            _, seq_len, d_model = x.shape
            assert seq_len <= self.pe.shape[1], "seq_len exceeds max_len."
            assert d_model == self.pe.shape[2], "d_model mismatch."
            # ç›´æ¥åŠ ä¸Šä½ç½®ç¼–ç 
            return x + self.pe[:, :seq_len, :]
    ```

3. Multi-Head Attention

    åœ¨ Transformer ä¸­, æ³¨æ„åŠ›æœºåˆ¶æ˜¯ä¸€ä¸ªéå¸¸é‡è¦çš„ç»„æˆéƒ¨åˆ†. å®ƒå…è®¸æ¨¡å‹åœ¨å¤„ç†è¾“å…¥åºåˆ—æ—¶, å…³æ³¨åºåˆ—ä¸­çš„ä¸åŒéƒ¨åˆ†. Multi-Head Attention æ˜¯ä¸€ç§å°†å¤šä¸ªæ³¨æ„åŠ›å¤´ï¼ˆattention headï¼‰ç»“åˆèµ·æ¥çš„æ–¹æ³•. æ¯ä¸ªæ³¨æ„åŠ›å¤´éƒ½æœ‰è‡ªå·±çš„å‚æ•°é›†, å¯ä»¥å­¦ä¹ åˆ°ä¸åŒçš„ç‰¹å¾è¡¨ç¤º. æœ€ç»ˆçš„è¾“å‡ºæ˜¯æ‰€æœ‰æ³¨æ„åŠ›å¤´çš„è¾“å‡ºæ‹¼æ¥åœ¨ä¸€èµ·. 

    <img src="figs/attention.png" height="300">

    å…¶å…¬å¼å¦‚ä¸‹æ‰€ç¤º: 

    $\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W^O$

    å…¶ä¸­, $Q, K, V$ åˆ†åˆ«è¡¨ç¤ºæŸ¥è¯¢ (Query)ã€é”® (Key) å’Œå€¼ (Value), $h$ è¡¨ç¤ºå¤´æ•°, $W^O$ æ˜¯ä¸€ä¸ªçº¿æ€§å˜æ¢çŸ©é˜µ. æ¯ä¸ªå¤´çš„è®¡ç®—å…¬å¼ä¸º: 

    $\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$

    å…¶ä¸­, $W_i^Q$, $W_i^K$, $W_i^V$ æ˜¯æ¯ä¸ªå¤´çš„çº¿æ€§å˜æ¢çŸ©é˜µ. æ³¨æ„åŠ›æœºåˆ¶çš„è®¡ç®—å…¬å¼ä¸º: 

    $\text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V$


    å…¶ä¸­, $d_k$ æ˜¯é”®çš„ç»´åº¦. 
    
    Multi-Head Attention çš„æ ¸å¿ƒä»£ç å®ç°å¦‚ä¸‹: 

    ```python
    class MultiHeadAttention(nn.Module):
        def __init__(self, d_model: int, heads: int, dropout: float = 0.1):
            super().__init__()

            ...
            self.INF = float(1e12)

        def forward(
            self,
            q: torch.Tensor,
            k: torch.Tensor,
            v: torch.Tensor,
            mask: Optional[torch.Tensor] = None,
        ) -> torch.Tensor:
            Q = rearrange(self.WQ(q), "b l (h d) -> b h l d", h=self.heads)
            K = rearrange(self.WK(k), "b l (h d) -> b h l d", h=self.heads)
            V = rearrange(self.WV(v), "b l (h d) -> b h l d", h=self.heads)
            dots = torch.matmul(Q, K.transpose(-2, -1)) / (
                self.dim_head**0.5
            )  # [batch_size, heads, q_len, k_len]
            if mask is not None:
                dots.masked_fill_(mask, -self.INF)
            attn = self.attend(dots)
            out = rearrange(
                torch.matmul(attn, V), "b h l d -> b l (h d)"
            )  # [batch_size, q_len, inner_dim]
            out = self.fc(self.dropout(out))
            return out
    ```
    > éœ€è¦ç‰¹åˆ«æ³¨æ„çš„æ˜¯, æ­¤å¤„åœ¨è®¾ç½® mask æ—¶, æˆ‘ä»¬æ²¡æœ‰ç›´æ¥ä½¿ç”¨ Python ä¸­è‡ªå¸¦çš„ inf, è€Œæ˜¯è®¾ç½®äº†ä¸€ä¸ªè‡ªå®šä¹‰çš„å¤§æ•°. è‹¥ç›´æ¥ä½¿ç”¨ inf, åœ¨è®¡ç®— Softmax å, ä¼šå‡ºç°å¯¹åº”ä½ç½®å€¼ä¸º nan çš„æƒ…å†µ. 

4. Feed Forward

    Feed Forward æ˜¯ Transformer ä¸­çš„ä¸€ä¸ªé‡è¦ç»„æˆéƒ¨åˆ†. å®ƒæ˜¯ä¸€ä¸ªä¸¤å±‚çš„å‰é¦ˆç¥ç»ç½‘ç»œ, é€šå¸¸ä½¿ç”¨ ReLU æ¿€æ´»å‡½æ•°. å…¶å…¬å¼å¦‚ä¸‹: 

    $\text{FFN}(x) = \text{ReLU}(xW_1 + b_1)W_2 + b_2$
    
    å…¶ä¸­, $W_1, b_1$ æ˜¯ç¬¬ä¸€å±‚çš„æƒé‡å’Œåç½®, $W_2, b_2$ æ˜¯ç¬¬äºŒå±‚çš„æƒé‡å’Œåç½®. Feed Forward çš„ä»£ç å®ç°å¦‚ä¸‹: 

    ```python
    class FeedForward(nn.Module):
        def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):
            super().__init__()
            self.linear1 = nn.Linear(d_model, d_ff)
            self.dropout = nn.Dropout(dropout)
            self.linear2 = nn.Linear(d_ff, d_model)
            self.relu = nn.ReLU()

        def forward(self, x: torch.Tensor) -> torch.Tensor:
            out = self.linear1(x)
            out = self.relu(out)
            out = self.dropout(out)
            out = self.linear2(out)
            return out
    ```

5. Encoder Layer

    <img src="figs/encoder_layer.png" height="300">

    æ¯ä¸ª Encoder Layer ä¸»è¦ç”±ä¸¤éƒ¨åˆ†ç»„æˆ: Multi-Head Attentionï¼ˆæ­¤å¤„çš„ Multi-Head Attention ä¸º Self-Attentionï¼‰ å’Œ Feed Forward. å®ƒä»¬ä¹‹é—´è¿˜æœ‰ä¸€ä¸ª Add & Norm çš„å¤„ç†, å³æ®‹å·®è¿æ¥ï¼ˆResidual Connectionï¼‰å’Œå±‚å½’ä¸€åŒ–ï¼ˆLayer Normalizationï¼‰. æ®‹å·®è¿æ¥å…è®¸æ¢¯åº¦åœ¨åå‘ä¼ æ’­æ—¶æ›´å®¹æ˜“åœ°æµè¿‡ç½‘ç»œ, ä»è€ŒåŠ é€Ÿè®­ç»ƒ. å±‚å½’ä¸€åŒ–ç”¨äºç¨³å®šè®­ç»ƒè¿‡ç¨‹. Encoder Layer çš„ä»£ç å®ç°å¦‚ä¸‹: 

    ```python
    class EncoderLayer(nn.Module):
        def __init__(self, d_model: int, heads: int, d_ff: int, dropout: float = 0.1):
            super().__init__()
            self.self_attention = MultiHeadAttention(d_model, heads, dropout)
            self.ffn = FeedForward(d_model, d_ff, dropout)
            self.dropout1 = nn.Dropout(dropout)
            self.ln1 = nn.LayerNorm(d_model)
            self.dropout2 = nn.Dropout(dropout)
            self.ln2 = nn.LayerNorm(d_model)

        def forward(
            self, x: torch.Tensor, mask: Optional[torch.Tensor] = None
        ) -> torch.Tensor:
            x = self.ln1(x + self.dropout1(self.self_attention(x, x, x, mask)))
            x = self.ln2(x + self.dropout2(self.ffn(x)))
            return x
    ```

6. Decoder Layer

    Decoder Layer ä¸ Encoder Layer ç±»ä¼¼. ä½†éœ€è¦æ³¨æ„çš„æ˜¯, Decoder Layer ä¸­ç¬¬ä¸€ä¸ª Multi-Head Attention ä¸º Self-Attention, å…¶ä¸­, Qã€Kã€V å‡æ¥è‡ª Decoder çš„è¾“å…¥ï¼›ç¬¬äºŒä¸ª Multi-Head Attention åˆ™ä¸º Cross-Attention, å…¶ä¸­, Q æ¥è‡ª Decoder çš„è¾“å…¥, Kã€V åˆ™ä¸º Encoder çš„è¾“å‡º. å…¶ä»£ç å®ç°å¦‚ä¸‹: 

    ```python
    class DecoderLayer(nn.Module):
        def __init__(self, d_model: int, heads: int, d_ff: int, dropout: float = 0.1):
            super().__init__()
            self.self_attention = MultiHeadAttention(d_model, heads, dropout)
            self.dropout1 = nn.Dropout(dropout)
            self.ln1 = nn.LayerNorm(d_model)
            self.cross_attention = MultiHeadAttention(d_model, heads, dropout)
            self.dropout2 = nn.Dropout(dropout)
            self.ln2 = nn.LayerNorm(d_model)
            self.ffn = FeedForward(d_model, d_ff, dropout)
            self.dropout3 = nn.Dropout(dropout)
            self.ln3 = nn.LayerNorm(d_model)

        def forward(
            self,
            x: torch.Tensor,
            encoder_kv: torch.Tensor,
            dst_mask: Optional[torch.Tensor] = None,
            src_dst_mask: Optional[torch.Tensor] = None,
        ) -> torch.Tensor:
            x = self.ln1(x + self.dropout1(self.self_attention(x, x, x, dst_mask)))
            x = self.ln2(
                x
                + self.dropout2(
                    self.cross_attention(x, encoder_kv, encoder_kv, src_dst_mask)
                )
            )
            x = self.ln3(x + self.dropout3(self.ffn(x)))
            return x
    ```

7. Encoder

    ç¼–ç å™¨ç”±å¤šä¸ªç¼–ç å™¨å±‚å †å è€Œæˆ. å…¶ä»£ç å®ç°å¦‚ä¸‹: 

    ```python
    class Encoder(nn.Module):
        def __init__(
            self,
            vocab_size: int,
            pad_idx: int,
            d_model: int,
            d_ff: int,
            n_layers: int,
            heads: int,
            dropout: float = 0.1,
            max_seq_len: int = 512,
        ):
            super().__init__()
            # pad ä¸å‚ä¸æ¢¯åº¦è®¡ç®—
            self.embedding = nn.Embedding(vocab_size, d_model, pad_idx)
            self.position_encoding = PositionEncoding(max_seq_len, d_model)
            self.layers = nn.ModuleList(
                [EncoderLayer(d_model, heads, d_ff, dropout) for _ in range(n_layers)]
            )
            self.dropout = nn.Dropout(dropout)

        def forward(self, x, src_mask: Optional[torch.Tensor] = None):
            x = self.embedding(x)
            x = self.position_encoding(x)
            x = self.dropout(x)
            for layer in self.layers:
                x = layer(x, src_mask)
            return x
    ```

8. Decoder

    è§£ç å™¨ç”±å¤šä¸ªè§£ç å™¨å±‚å †å è€Œæˆ. å…¶ä»£ç å®ç°å¦‚ä¸‹: 

    ```python
    class Decoder(nn.Module):
        def __init__(
            self,
            vocab_size: int,
            pad_idx: int,
            d_model: int,
            d_ff: int,
            n_layers: int,
            heads: int,
            dropout: float = 0.1,
            max_seq_len: int = 512,
        ):
            super().__init__()
            self.embedding = nn.Embedding(vocab_size, d_model, pad_idx)
            self.position_encoding = PositionEncoding(max_seq_len, d_model)
            self.layers = nn.ModuleList(
                [DecoderLayer(d_model, heads, d_ff, dropout) for _ in range(n_layers)]
            )
            self.dropout = nn.Dropout(dropout)

        def forward(
            self,
            x: torch.Tensor,
            encoder_kv,
            dst_mask: Optional[torch.Tensor] = None,
            src_dst_mask: Optional[torch.Tensor] = None,
        ):
            x = self.embedding(x)
            x = self.position_encoding(x)
            x = self.dropout(x)
            for layer in self.layers:
                x = layer(x, encoder_kv, dst_mask, src_dst_mask)
            return x
    ```

æœ‰äº†ä»¥ä¸Šçš„ç»„ä»¶, æˆ‘ä»¬å°±å¯ä»¥å°†å®ƒä»¬ç»„åˆæˆå®Œæ•´çš„ Transformer æ¨¡å‹äº†. Transformer æ¨¡å‹çš„ä»£ç å®ç°å¦‚ä¸‹: 

```python
class Transformer(nn.Module):
    def __init__(
        self,
        src_vocab_size: int,
        dst_vocab_size: int,
        pad_idx: int,
        d_model: int,
        d_ff: int,
        n_layers: int,
        heads: int,
        dropout: float = 0.1,
        max_seq_len: int = 512,
    ):
        super().__init__()
        self.encoder = Encoder(
            src_vocab_size,
            pad_idx,
            d_model,
            d_ff,
            n_layers,
            heads,
            dropout,
            max_seq_len,
        )
        self.decoder = Decoder(
            dst_vocab_size,
            pad_idx,
            d_model,
            d_ff,
            n_layers,
            heads,
            dropout,
            max_seq_len,
        )
        self.pad_idx = pad_idx
        self.output_layer = nn.Linear(d_model, dst_vocab_size)

    def generate_mask(
        self, q_pad: torch.Tensor, k_pad: torch.Tensor, apply_causal_mask: bool = False
    ):
        n, q_len = q_pad.shape
        n, k_len = k_pad.shape

        mask_shape = (n, 1, q_len, k_len)
        if apply_causal_mask:
            # Decoder mask
            mask = 1 - torch.tril(
                torch.ones(mask_shape)
            )  # å¯¹è§’çº¿ä»¥ä¸Šå…¨ä¸º 1, å³å±è”½ä¹‹å‰çš„ä¿¡æ¯

        else:
            # Encoder mask
            mask = torch.zeros(mask_shape)
        mask = mask.to(q_pad.device)
        for i in range(n):
            mask[i, :, q_pad[i], :] = 1
            mask[i, :, :, k_pad[i]] = 1
        mask = mask.to(torch.bool)
        return mask

    def forward(self, x, y):
        src_pad_mask = x == self.pad_idx
        dst_pad_mask = y == self.pad_idx
        src_mask = self.generate_mask(
            q_pad=src_pad_mask, k_pad=src_pad_mask, apply_causal_mask=False
        )  # ä½¿ç”¨ encoder mask
        dst_mask = self.generate_mask(
            q_pad=dst_pad_mask, k_pad=dst_pad_mask, apply_causal_mask=True
        )  # ä½¿ç”¨ decoder mask
        src_dst_mask = self.generate_mask(
            q_pad=dst_pad_mask, k_pad=src_pad_mask, apply_causal_mask=False
        )
        encoder_kv = self.encoder(x, src_mask)
        res = self.decoder(y, encoder_kv, dst_mask, src_dst_mask)
        res = self.output_layer(res)
        return res
```

é™¤äº†å·²æœ‰çš„å°ç»„ä»¶ä¹‹å¤–, å¯ä»¥çœ‹åˆ° Transformer ä¸­è¿˜åŒ…æ‹¬äº†ä¸€ä¸ªå¾ˆé‡è¦çš„å‡½æ•° `generate_mask`, è¯¥å‡½æ•°ç”¨äºç”Ÿæˆ padding mask å’Œ causal mask. padding mask ç”¨äºå±è”½æ‰è¾“å…¥åºåˆ—ä¸­çš„å¡«å……éƒ¨åˆ†, è€Œ causal mask åˆ™ç”¨äºå±è”½æ‰è§£ç å™¨ä¸­å½“å‰ token ä¹‹åçš„éƒ¨åˆ†. è¿™æ ·å¯ä»¥ç¡®ä¿æ¨¡å‹åœ¨ç”Ÿæˆä¸‹ä¸€ä¸ª token æ—¶, åªèƒ½çœ‹åˆ°å½“å‰ token ä¹‹å‰çš„éƒ¨åˆ†. 

è‡³æ­¤, æ¨¡å‹å°±å·²æ­å»ºå®Œæ¯•. 

### æ•°æ®å¤„ç†

æœ¬æ¬¡å®éªŒçš„ä»»åŠ¡æ˜¯å°†ä¸­æ–‡ç¿»è¯‘æˆè‹±æ–‡, ä½¿ç”¨çš„æ•°æ®é›†åŒ…å«äº† 21621 æ¡ä¸­è‹±å¹³è¡Œè¯­æ–™, æ¯å¯¹è¯­æ–™ä¸­åŒ…å«ä¸€æ¡è‹±æ–‡è¾“å…¥è¯­å¥åŠå…¶å¯¹åº”çš„ä¸­æ–‡ç¿»è¯‘ç»“æœ, è‹±æ–‡å’Œä¸­æ–‡ä¹‹é—´ä½¿ç”¨åˆ¶è¡¨ç¬¦ (tab) è¿›è¡Œåˆ†éš”. ä¾‹å¦‚: 

```txt
it 's none of your concern . 	è¿™ä¸å…³ ä½  çš„ äº‹ ã€‚
```

åœ¨æ•°æ®å¤„ç†é˜¶æ®µ, æˆ‘ä»¬éœ€è¦å°†è‡ªç„¶è¯­è¨€è½¬æ¢ä¸ºè®¡ç®—æœºå¯ä»¥ç†è§£çš„æ•°å€¼å½¢å¼, å³ token åºåˆ—. åœ¨ `cmn-eng-simple` æ•°æ®é›†å½“ä¸­, é¢„å¤„ç†æ—¶ä½¿ç”¨ jieba åˆ†è¯å™¨ç»™ä¸­æ–‡æ–‡æœ¬è¿›è¡Œåˆ†è¯, ä½¿ç”¨ BPE åˆ†è¯å™¨ç»™è‹±æ–‡è¿›è¡Œåˆ†è¯, å¹¶é¢„å…ˆå®šä¹‰å¥½äº†è‡ªç„¶è¯­è¨€è¯è¯­ä¸ token ä¹‹é—´çš„ä¸€å¯¹ä¸€æ˜ å°„å…³ç³» (å³è¯è¡¨), å…·ä½“å¯è§ `int2word_cn.json`ã€`int2word_en.json`ã€`word2int_cn.json`ã€`word2int_en.json` å››ä¸ªæ–‡ä»¶. å…¶ä¸­, è‹±æ–‡è¯è¡¨çš„å¤§å°ä¸º 3922, ä¸­æ–‡è¯è¡¨çš„å¤§å°ä¸º 3775. æˆ‘ä»¬å¯ä»¥ç›´æ¥ä½¿ç”¨è¿™äº›æ–‡ä»¶æ¥è¿›è¡Œæ•°æ®å¤„ç†. ä¸ºäº†æ–¹ä¾¿åç»­çš„è®­ç»ƒå’Œè¯„æµ‹æ—¶æŒ‰ç…§ batch è¿›è¡Œå¤„ç†, æˆ‘ä»¬åŒæ—¶å°†æ•°æ®é›†å°è£…æˆäº† `TranslationDataset` ç±», ç»§æ‰¿è‡ª `torch.utils.data.Dataset` ç±». å½“æˆ‘ä»¬å®ä¾‹åŒ– `TranslationDataset` ç±»æ—¶, ä¼šè‡ªåŠ¨åŠ è½½è¯è¡¨, å¹¶æ ¹æ®è¯è¡¨å°†è‡ªç„¶è¯­è¨€è½¬æ¢ä¸º token åºåˆ—, å¹¶å¯é€šè¿‡ä¸‹æ ‡è®¿é—®åˆ°è½¬æ¢æˆ `torch.LongTensor` ç±»å‹çš„è‹±æ–‡å’Œä¸­æ–‡ token åºåˆ—. å…·ä½“ä»£ç å¯è§ `dataset.py` æ–‡ä»¶. 

æ„å»ºæ•°æ®é›†å, æˆ‘ä»¬å¯ä»¥é€šè¿‡å¦‚ä¸‹æ–¹å¼æ£€éªŒèƒ½å¦æ­£ç¡®åŠ è½½æ•°æ®é›†: 

```python
if __name__ == "__main__":
    data_dir = "data/cmn-eng-simple"
    dataset = TranslationDataset(data_dir, split="train")
    print(f"Number of samples: {len(dataset)}\n")
    for i in range(3):
        en, cn = dataset[i]
        print("English: ", end=" ")
        for word in en:
            print(dataset.int2en[str(word.item())], end=" ")
        print("\nChinese: ", end=" ")
        for word in cn:
            print(dataset.int2cn[str(word.item())], end=" ")
        print("\n")
```

è¾“å‡ºç»“æœå¦‚ä¸‹æ‰€ç¤º: 

```txt
Number of samples: 18000

English:  <BOS> it 's none of your concern . <EOS> 
Chinese:  <BOS> è¿™ä¸å…³ ä½  çš„ äº‹ .  <EOS> 

English:  <BOS> she has a habit of <UNK> ting her na ils . <EOS> 
Chinese:  <BOS> å¥¹ æœ‰ å’¬ <UNK> çš„ ä¹ æƒ¯ .  <EOS> 

English:  <BOS> he is a teacher . <EOS> 
Chinese:  <BOS> ä»– æ˜¯ è€å¸ˆ .  <EOS>  
```
æ•…æ•°æ®é›†åŠ è½½æˆåŠŸ. 

### è¶…å‚æ•°è®¾ç½®

æœ¬æ¬¡å®éªŒä¸­, æˆ‘ä»¬é€šè¿‡ `config.py` æ–‡ä»¶æ¥ç®¡ç†è¶…å‚æ•°, å…·ä½“å¦‚ä¸‹: 

| å‚æ•°å         | é»˜è®¤å€¼         | è¯´æ˜               |
|----------------|---------------|--------------------|
| seed           | 2025          | éšæœºç§å­           |
| d_model        | 512           | æ¨¡å‹å†…éƒ¨ç»´åº¦       |
| d_ff           | 2048          | å‰é¦ˆç½‘ç»œç»´åº¦       |
| n_layers       | 6             | ç¼–ç å™¨å’Œè§£ç å™¨å±‚æ•° |
| heads          | 8             | æ³¨æ„åŠ›å¤´æ•°         |
| dropout        | 0.1           | dropout æ¦‚ç‡       |
| max_seq_len    | 100           | æœ€å¤§åºåˆ—é•¿åº¦       |
| batch_size     | 16            | æ‰¹æ¬¡å¤§å°           |
| lr             | 1e-4          | å­¦ä¹ ç‡             |
| n_epochs       | 60            | è®­ç»ƒè½®æ•°           |
| print_interval | 50            | æ‰“å°é—´éš”           |

å…¶ä¸­, éšæœºç§å­ç”¨äºä¿è¯å®éªŒç»“æœçš„å¯é‡å¤æ€§, æ¨¡å‹å†…éƒ¨ç»´åº¦ã€å‰é¦ˆç½‘ç»œç»´åº¦ã€ç¼–ç å™¨å’Œè§£ç å™¨å±‚æ•°ã€æ³¨æ„åŠ›å¤´æ•°ã€dropout æ¦‚ç‡ã€æœ€å¤§åºåˆ—é•¿åº¦ã€æ‰¹æ¬¡å¤§å°ã€å­¦ä¹ ç‡ã€è®­ç»ƒè½®æ•°ã€æ‰“å°é—´éš”ç­‰å‚æ•°åˆ™ç”¨äºæ§åˆ¶æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹å’Œæ€§èƒ½. æˆ‘ä»¬å¯ä»¥æ‰“å°å‡ºæ¨¡å‹çš„ç»“æ„å’Œå‚æ•°é‡: 

```txt
    +-------------------------------------------------------+----------------+------------+-------------+
    | Layer (type)                                          | Output Shape   | Param #    | Trainable   |
    |-------------------------------------------------------+----------------+------------+-------------|
    | encoder (Encoder)                                     | [--]           | 20,922,368 | Yes         |
    | encoder.embedding (Embedding)                         | [--]           | 2,008,064  | Yes         |
    | encoder.layers (ModuleList)                           | [--]           | 18,914,304 | Yes         |
    | encoder.layers.0 (EncoderLayer)                       | [--]           | 3,152,384  | Yes         |
    | encoder.layers.0.self_attention (MultiHeadAttention)  | [--]           | 1,050,624  | Yes         |
    | encoder.layers.0.ffn (FeedForward)                    | [--]           | 2,099,712  | Yes         |
    | encoder.layers.0.ln1 (LayerNorm)                      | [--]           | 1,024      | Yes         |
    | encoder.layers.0.ln2 (LayerNorm)                      | [--]           | 1,024      | Yes         |
    | encoder.layers.1 (EncoderLayer)                       | [--]           | 3,152,384  | Yes         |
    | encoder.layers.1.self_attention (MultiHeadAttention)  | [--]           | 1,050,624  | Yes         |
    | encoder.layers.1.ffn (FeedForward)                    | [--]           | 2,099,712  | Yes         |
    | encoder.layers.1.ln1 (LayerNorm)                      | [--]           | 1,024      | Yes         |
    | encoder.layers.1.ln2 (LayerNorm)                      | [--]           | 1,024      | Yes         |
    | encoder.layers.2 (EncoderLayer)                       | [--]           | 3,152,384  | Yes         |
    | encoder.layers.2.self_attention (MultiHeadAttention)  | [--]           | 1,050,624  | Yes         |
    | encoder.layers.2.ffn (FeedForward)                    | [--]           | 2,099,712  | Yes         |
    | encoder.layers.2.ln1 (LayerNorm)                      | [--]           | 1,024      | Yes         |
    | encoder.layers.2.ln2 (LayerNorm)                      | [--]           | 1,024      | Yes         |
    | encoder.layers.3 (EncoderLayer)                       | [--]           | 3,152,384  | Yes         |
    | encoder.layers.3.self_attention (MultiHeadAttention)  | [--]           | 1,050,624  | Yes         |
    | encoder.layers.3.ffn (FeedForward)                    | [--]           | 2,099,712  | Yes         |
    | encoder.layers.3.ln1 (LayerNorm)                      | [--]           | 1,024      | Yes         |
    | encoder.layers.3.ln2 (LayerNorm)                      | [--]           | 1,024      | Yes         |
    | encoder.layers.4 (EncoderLayer)                       | [--]           | 3,152,384  | Yes         |
    | encoder.layers.4.self_attention (MultiHeadAttention)  | [--]           | 1,050,624  | Yes         |
    | encoder.layers.4.ffn (FeedForward)                    | [--]           | 2,099,712  | Yes         |
    | encoder.layers.4.ln1 (LayerNorm)                      | [--]           | 1,024      | Yes         |
    | encoder.layers.4.ln2 (LayerNorm)                      | [--]           | 1,024      | Yes         |
    | encoder.layers.5 (EncoderLayer)                       | [--]           | 3,152,384  | Yes         |
    | encoder.layers.5.self_attention (MultiHeadAttention)  | [--]           | 1,050,624  | Yes         |
    | encoder.layers.5.ffn (FeedForward)                    | [--]           | 2,099,712  | Yes         |
    | encoder.layers.5.ln1 (LayerNorm)                      | [--]           | 1,024      | Yes         |
    | encoder.layers.5.ln2 (LayerNorm)                      | [--]           | 1,024      | Yes         |
    | decoder (Decoder)                                     | [--]           | 27,156,992 | Yes         |
    | decoder.embedding (Embedding)                         | [--]           | 1,932,800  | Yes         |
    | decoder.layers (ModuleList)                           | [--]           | 25,224,192 | Yes         |
    | decoder.layers.0 (DecoderLayer)                       | [--]           | 4,204,032  | Yes         |
    | decoder.layers.0.self_attention (MultiHeadAttention)  | [--]           | 1,050,624  | Yes         |
    | decoder.layers.0.ln1 (LayerNorm)                      | [--]           | 1,024      | Yes         |
    | decoder.layers.0.cross_attention (MultiHeadAttention) | [--]           | 1,050,624  | Yes         |
    | decoder.layers.0.ln2 (LayerNorm)                      | [--]           | 1,024      | Yes         |
    | decoder.layers.0.ffn (FeedForward)                    | [--]           | 2,099,712  | Yes         |
    | decoder.layers.0.ln3 (LayerNorm)                      | [--]           | 1,024      | Yes         |
    | decoder.layers.1 (DecoderLayer)                       | [--]           | 4,204,032  | Yes         |
    | decoder.layers.1.self_attention (MultiHeadAttention)  | [--]           | 1,050,624  | Yes         |
    | decoder.layers.1.ln1 (LayerNorm)                      | [--]           | 1,024      | Yes         |
    | decoder.layers.1.cross_attention (MultiHeadAttention) | [--]           | 1,050,624  | Yes         |
    | decoder.layers.1.ln2 (LayerNorm)                      | [--]           | 1,024      | Yes         |
    | decoder.layers.1.ffn (FeedForward)                    | [--]           | 2,099,712  | Yes         |
    | decoder.layers.1.ln3 (LayerNorm)                      | [--]           | 1,024      | Yes         |
    | decoder.layers.2 (DecoderLayer)                       | [--]           | 4,204,032  | Yes         |
    | decoder.layers.2.self_attention (MultiHeadAttention)  | [--]           | 1,050,624  | Yes         |
    | decoder.layers.2.ln1 (LayerNorm)                      | [--]           | 1,024      | Yes         |
    | decoder.layers.2.cross_attention (MultiHeadAttention) | [--]           | 1,050,624  | Yes         |
    | decoder.layers.2.ln2 (LayerNorm)                      | [--]           | 1,024      | Yes         |
    | decoder.layers.2.ffn (FeedForward)                    | [--]           | 2,099,712  | Yes         |
    | decoder.layers.2.ln3 (LayerNorm)                      | [--]           | 1,024      | Yes         |
    | decoder.layers.3 (DecoderLayer)                       | [--]           | 4,204,032  | Yes         |
    | decoder.layers.3.self_attention (MultiHeadAttention)  | [--]           | 1,050,624  | Yes         |
    | decoder.layers.3.ln1 (LayerNorm)                      | [--]           | 1,024      | Yes         |
    | decoder.layers.3.cross_attention (MultiHeadAttention) | [--]           | 1,050,624  | Yes         |
    | decoder.layers.3.ln2 (LayerNorm)                      | [--]           | 1,024      | Yes         |
    | decoder.layers.3.ffn (FeedForward)                    | [--]           | 2,099,712  | Yes         |
    | decoder.layers.3.ln3 (LayerNorm)                      | [--]           | 1,024      | Yes         |
    | decoder.layers.4 (DecoderLayer)                       | [--]           | 4,204,032  | Yes         |
    | decoder.layers.4.self_attention (MultiHeadAttention)  | [--]           | 1,050,624  | Yes         |
    | decoder.layers.4.ln1 (LayerNorm)                      | [--]           | 1,024      | Yes         |
    | decoder.layers.4.cross_attention (MultiHeadAttention) | [--]           | 1,050,624  | Yes         |
    | decoder.layers.4.ln2 (LayerNorm)                      | [--]           | 1,024      | Yes         |
    | decoder.layers.4.ffn (FeedForward)                    | [--]           | 2,099,712  | Yes         |
    | decoder.layers.4.ln3 (LayerNorm)                      | [--]           | 1,024      | Yes         |
    | decoder.layers.5 (DecoderLayer)                       | [--]           | 4,204,032  | Yes         |
    | decoder.layers.5.self_attention (MultiHeadAttention)  | [--]           | 1,050,624  | Yes         |
    | decoder.layers.5.ln1 (LayerNorm)                      | [--]           | 1,024      | Yes         |
    | decoder.layers.5.cross_attention (MultiHeadAttention) | [--]           | 1,050,624  | Yes         |
    | decoder.layers.5.ln2 (LayerNorm)                      | [--]           | 1,024      | Yes         |
    | decoder.layers.5.ffn (FeedForward)                    | [--]           | 2,099,712  | Yes         |
    | decoder.layers.5.ln3 (LayerNorm)                      | [--]           | 1,024      | Yes         |
    | output_layer (Linear)                                 | [--]           | 1,936,575  | Yes         |
    +-------------------------------------------------------+----------------+------------+-------------+

    ============================================================
    Total params: 186.37M (186,372,287)
    Trainable params: 186.37M (186,372,287)
    Non-trainable params: 0 (0)
    Model size: 710.95MB (FP32)
    ============================================================
```

å¯ä»¥çœ‹åˆ°æ¨¡å‹çš„å‚æ•°é‡å¤§çº¦ä¸º 1.86 äº¿, æ¨¡å‹å¤§å°å¤§çº¦ä¸º 710MB. 

### æ¨¡å‹è®­ç»ƒåŠè¯„ä¼°

è¯¥éƒ¨åˆ†çš„ä»£ç ä¸å¸¸è§„æ·±åº¦å­¦ä¹ è®­ç»ƒè¿‡ç¨‹ç±»ä¼¼, æ•…ä¸å†èµ˜è¿°, éœ€è¦æ³¨æ„çš„ç»†èŠ‚å¯è§æ³¨é‡Š, ä»£ç å¯è§ `main.py` æ–‡ä»¶. 

è®­ç»ƒå®Œæˆå, æˆ‘ä»¬å¯ä»¥æŸ¥çœ‹ä¸€ä¸‹è®­ç»ƒçš„æŸå¤±å’Œå‡†ç¡®ç‡çš„å˜åŒ–æ›²çº¿. å…¶ä¸­, è®­ç»ƒé›†å’ŒéªŒè¯é›†ä¸Šçš„ Loss ä»¥åŠ Accuracy çš„å˜åŒ–æ›²çº¿å¦‚ä¸‹å›¾æ‰€ç¤º: 

![alt text](figs/accuracy_curve.png)

![alt text](figs/loss_curve.png)

ä»å›¾ä¸­å¯ä»¥å‘ç°, è™½ç„¶éšç€è®­ç»ƒ epoch çš„å¢åŠ , è®­ç»ƒé›†çš„ Loss æŒç»­ä¸‹é™, Accuracy æŒç»­ä¸Šå‡, ä½†æ˜¯åœ¨ç¬¬ 10 ä¸ª epoch ä¹‹å, éªŒè¯é›†çš„ Loss å°±å¼€å§‹ä¸Šå‡, Accuracy å¼€å§‹ä¸‹é™, è¿™è¯´æ˜æ¨¡å‹å‡ºç°äº†è¿‡æ‹Ÿåˆçš„ç°è±¡. å› æ­¤, åœ¨è¯¥è¶…å‚æ•°è®¾ç½®ä¸‹, æ¨¡å‹çš„æœ€ä½³æ•ˆæœå‡ºç°åœ¨ç¬¬ 10 ä¸ª epoch, æ­¤æ—¶éªŒè¯é›†çš„ Loss æœ€å°, Accuracy æœ€å¤§. æˆ‘ä»¬å¯ä»¥å°†è¯¥ epoch çš„æ¨¡å‹ä¿å­˜ä¸‹æ¥, ä½œä¸ºæœ€ç»ˆçš„æ¨¡å‹. 

5. æ¨¡å‹è¯„ä¼°

    åœ¨æ¨¡å‹è¯„ä¼°æ—¶, æˆ‘ä»¬ä½¿ç”¨ä¸Šæ–‡æåˆ°çš„è®­ç»ƒäº† 10 ä¸ª epoch çš„æ¨¡å‹, å¹¶ä½¿ç”¨ BLEU åˆ†æ•°æ¥è¯„ä¼°æ¨¡å‹çš„ç¿»è¯‘æ•ˆæœ. BLEU åˆ†æ•°æ˜¯ä¸€ä¸ªå¸¸ç”¨çš„æœºå™¨ç¿»è¯‘è¯„ä¼°æŒ‡æ ‡, ä¸»è¦ç”¨äºè¡¡é‡æœºå™¨ç¿»è¯‘ç»“æœä¸å‚è€ƒç¿»è¯‘ä¹‹é—´çš„ç›¸ä¼¼åº¦. BLEU åˆ†æ•°è¶Šé«˜, è¡¨ç¤ºç¿»è¯‘ç»“æœè¶Šå¥½. 
    åœ¨è¿™é‡Œ, æˆ‘ä»¬ä½¿ç”¨ `nltk.translate.bleu_score` åº“ä¸­çš„ `corpus_bleu` å‡½æ•°æ¥è®¡ç®— BLEU åˆ†æ•°. ä»£ç å®ç°å¦‚ä¸‹: 

    ```python
    from nltk.translate.bleu_score import corpus_bleu

    # ......
    elif period == "eval":
        model_path = os.path.join(output_dir, "final_model.pth")
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"Model file {model_path} not found.")
        model.load_state_dict(torch.load(model_path, weights_only=True))
        model.eval()
        with torch.no_grad():
            en_origin = []
            cn_standard = []
            cn_output = []
            for batch in tqdm(test_loader):
                x = torch.LongTensor(batch["source"]).to(device)
                y = torch.LongTensor(batch["target"]).to(device)
                batch_size = x.shape[0]
                max_len = y.shape[1]
                y_output = torch.full(
                    (batch_size, max_len), PAD_ID, dtype=torch.long, device=device
                )
                y_output[:, 0] = BOS_ID
                for cur_idx in range(1, max_len):
                    decoder_input = y_output[:, :cur_idx]
                    output = model(x, decoder_input)
                    next_tokens = torch.argmax(output[:, -1, :], dim=-1)
                    y_output[:, cur_idx] = next_tokens
                for j in range(batch_size):
                    en_origin.append(convert_to_text(x[j], int2en))
                    cn_standard.append(convert_to_text(y[j], int2cn))
                    cn_output.append(convert_to_text(y_output[j], int2cn))
        references = [[ref.split()] for ref in cn_standard]
        hypotheses = [hyp.split() for hyp in cn_output]
        bleu_score = corpus_bleu(references, hypotheses)
        print(f"BLEU Score: {bleu_score:.4f}")
    ```

### æ¡ˆä¾‹åˆ†æ

æ­¤å¤–, æˆ‘ä»¬è¿˜å¯ä»¥æ‰‹åŠ¨æ£€éªŒæ¨¡å‹çš„ç¿»è¯‘æ•ˆæœ, ä»£ç å®ç°å¦‚ä¸‹: 

```python
print("-" * 50)
for i in range(3):
        print(f"Original: {en_origin[i]}")
        print(f"Standard: {cn_standard[i]}")
        print(f"Translated: {cn_output[i]}")
        print("-" * 50)
```

æœ€ç»ˆçš„è¾“å‡ºç»“æœå¦‚ä¸‹: 

```txt
BLEU Score: 0.2766
--------------------------------------------------
Original: do you still want to talk to me ?
Standard Answer: ä½  è¿˜ æƒ³ è·Ÿ æˆ‘ è°ˆ å— ï¼Ÿ
Translated: ä½  è¿˜ æƒ³ è·Ÿ æˆ‘ è¯´ å— ï¼Ÿ
--------------------------------------------------
Original: i will never for ce you to marry him .
Standard Answer: æˆ‘æ°¸è¿œ ä¸ä¼š é€¼ ä½  è·Ÿ ä»– ç»“å©š . 
Translated: æˆ‘ ä¸ä¼š å¿˜è®° ä½  å’Œ ä»– ç»“å©š äº† . 
--------------------------------------------------
Original: i 'm going to go tell tom .
Standard Answer: æˆ‘è¦ å‘Šè¯‰ æ±¤å§† . 
Translated: æˆ‘è¦ å‘Šè¯‰ æ±¤å§† . 
--------------------------------------------------
```

BLEU åˆ†æ•°ä¸º 0.2322, è¯´æ˜æ¨¡å‹çš„ç¿»è¯‘æ•ˆæœè¿˜ä¸é”™. åŒæ—¶, æˆ‘ä»¬å¯ä»¥çœ‹åˆ°, æ‰‹åŠ¨è¾“å‡ºçš„å‡ ä¸ªç¤ºä¾‹ä¸­, æ¨¡å‹çš„ç¿»è¯‘æ•ˆæœä¹Ÿè¾ƒä¸ºç†æƒ³. è™½ç„¶æœ‰äº›åœ°æ–¹ç¿»è¯‘å¾—ä¸æ˜¯å¾ˆæ­£ç¡®, ä¾‹å¦‚å°† "force" ç¿»è¯‘æˆäº† "å¿˜è®°", ä½†æ•´ä½“ä¸Šè¿˜æ˜¯ç¬¦åˆé€»è¾‘ä¸”è¾ƒä¸ºå‡†ç¡®çš„. 

### æ¶ˆèå®éªŒ

åœ¨æ¶ˆèå®éªŒéƒ¨åˆ†, æˆ‘ä¸»è¦ä»æ¨¡å‹ç»“æ„æ–¹é¢è¿›è¡Œäº†æ¶ˆèå®éªŒ, ä¸»è¦æ¢ç©¶äº†ä¸åŒçš„æ³¨æ„åŠ›å¤´æ•°ã€ä¸åŒçš„ç¼–ç å™¨å’Œè§£ç å™¨å±‚æ•°å’Œæ˜¯å¦ä½¿ç”¨ä½ç½®ç¼–ç å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“. 

#### æ³¨æ„åŠ›å¤´æ•°

åœ¨ Transformer æ¨¡å‹ä¸­, æ³¨æ„åŠ›å¤´æ•°æ˜¯ä¸€ä¸ªé‡è¦çš„è¶…å‚æ•°, å®ƒå†³å®šäº†æ¨¡å‹åœ¨å¤„ç†åºåˆ—æ•°æ®æ—¶, æ˜¯å¦èƒ½å¤Ÿæ•æ‰åˆ°ä¸åŒä½ç½®ä¹‹é—´çš„ä¾èµ–å…³ç³». å½“å¤´æ•°è¾ƒå¤šæ—¶, æ¨¡å‹èƒ½å¤Ÿå¹¶è¡Œåœ°åœ¨å­ç©ºé—´ä¸­æ•è·ä¸åŒä½ç½®ä¹‹é—´çš„ä¾èµ–å…³ç³», ä»è€Œæé«˜æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›, ä½†åŒæ—¶ä¹Ÿå¯èƒ½å› å½±å“å­ç©ºé—´çš„å¤§å°è€Œé™ä½æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›. å½“å¤´æ•°è¿‡å°‘æ—¶, æ¨¡å‹å¯èƒ½æ— æ³•æ•æ‰åˆ°è¶³å¤Ÿçš„ä¿¡æ¯, ä»è€Œå½±å“æ¨¡å‹çš„æ€§èƒ½. å› æ­¤, æˆ‘ä»¬éœ€è¦æ¢ç©¶æ³¨æ„åŠ›å¤´æ•°å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“. 

åœ¨æœ¬æ¬¡å®éªŒä¸­, æˆ‘è®¾ç½®äº† 5 ä¸ªä¸åŒçš„æ³¨æ„åŠ›å¤´æ•°, åˆ†åˆ«ä¸º 1, 2, 4, 8, 16, å¹¶è®¡ç®—å¯¹åº”çš„ BLEU åˆ†æ•°. å®éªŒç»“æœå¦‚ä¸‹: 

| æ³¨æ„åŠ›å¤´æ•° | BLEU åˆ†æ•° |
|------------|-----------|
| 1          | 0.2307    |
| 2          | 0.2371    |
| 4          | 0.2438    |
| 8          | **0.2766**    |
| 16         | 0.2408    |

ä»å®éªŒç»“æœå¯ä»¥çœ‹å‡º, å½“æ³¨æ„åŠ›å¤´æ•°ä¸º 8 æ—¶, æ¨¡å‹çš„ BLEU åˆ†æ•°æœ€é«˜, ä¸º 0.2766. å½“æ³¨æ„åŠ›å¤´æ•°ä¸º 1 æ—¶, æ¨¡å‹çš„ BLEU åˆ†æ•°æœ€ä½, ä¸º 0.2307. æ³¨æ„åŠ›å¤´æ•°ä¸º 16 æ—¶, æ¨¡å‹çš„ BLEU åˆ†æ•°ä¸º 0.2408, ç•¥ä½äºæ³¨æ„åŠ›å¤´æ•°ä¸º 8 æ—¶çš„ BLEU åˆ†æ•°. ç”±æ­¤ä¹Ÿå¯ä»¥çœ‹å‡º, æ³¨æ„åŠ›å¤´æ•°å¹¶éè¶Šå¤šè¶Šå¥½, æˆ–è¶Šå°‘è¶Šå¥½, è€Œæ˜¯éœ€è¦æ ¹æ®å…·ä½“ä»»åŠ¡å’Œæ•°æ®é›†è¿›è¡Œè°ƒæ•´. 

#### ç¼–ç å™¨å’Œè§£ç å™¨å±‚æ•°

åœ¨ Transformer æ¨¡å‹ä¸­, ç¼–ç å™¨å’Œè§£ç å™¨å±‚æ•°ä¹Ÿæ˜¯ååˆ†é‡è¦çš„è¶…å‚æ•°. æ¨¡å‹çš„å±‚æ•°ç›´æ¥å½±å“æ·±åº¦å’Œå¤§å°, ä»è€Œå½±å“æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›. 

åœ¨æœ¬æ¬¡å®éªŒä¸­, æˆ‘ä¹Ÿè®¾ç½®äº† 5 ä¸ªä¸åŒçš„ç¼–ç å™¨å’Œè§£ç å™¨å±‚æ•°, åˆ†åˆ«ä¸º 1, 2, 4, 6, 8, 10, å¹¶è®¡ç®—å¯¹åº”çš„ BLEU åˆ†æ•°. å®éªŒç»“æœå¦‚ä¸‹: 

| ç¼–ç å™¨å’Œè§£ç å™¨å±‚æ•° | BLEU åˆ†æ•° |
|------------------|-----------|
| 1                | 0.2134    |
| 2                | 0.2329    |
| 4                | 0.2428    |
| 6                | **0.2766**    |
| 8                | 0.2594    |
| 10               | 0.2293    |

ä»å®éªŒç»“æœå¯ä»¥çœ‹å‡º, å½“å±‚æ•°ä¸º 6 æ—¶, BLEU åˆ†æ•°æœ€é«˜. è€Œå±‚æ•°ä¸º 10 æ—¶, æ•ˆæœç”šè‡³ä¸å¦‚å±‚æ•°ä¸º 2 æ—¶çš„æ•ˆæœ. è¿™å¯èƒ½æ˜¯å› ä¸ºå±‚æ•°è¿‡å¤šæ—¶, æ¨¡å‹å¯èƒ½å‡ºç°è¿‡æ‹Ÿåˆçš„ç°è±¡, ä»è€Œå½±å“æ¨¡å‹çš„æ€§èƒ½. 

### ä½ç½®ç¼–ç 

ä½ç½®ç¼–ç å¯¹äº Transformer æ¨¡å‹æ¥è¯´, æ˜¯ä¸€ä¸ªååˆ†é‡è¦çš„æ¨¡å—. ä½ç½®ç¼–ç çš„ç›®çš„æ˜¯ä¸ºäº†å°†åºåˆ—ä¸­çš„ä½ç½®ä¿¡æ¯ç¼–ç æˆå‘é‡, ä»è€Œè®©æ¨¡å‹èƒ½å¤Ÿæ•æ‰åˆ°åºåˆ—ä¸­çš„é¡ºåºä¿¡æ¯. è‹¥æ²¡æœ‰ä½ç½®ç¼–ç , æ¨¡å‹åœ¨å¹¶è¡ŒåŒ–å¤„ç†æ—¶, ä¼šä¸¢å¤±åºåˆ—çš„é¡ºåºä¿¡æ¯, ä»è€Œå½±å“æ¨¡å‹çš„æ€§èƒ½. å› æ­¤, æˆ‘ä»¬éœ€è¦æ¢ç©¶ä½ç½®ç¼–ç å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“. 

| ä½ç½®ç¼–ç  | BLEU åˆ†æ•° |
|------|-----------|
| âŒ | 0.2175    |
| âœ… | **0.2766**    |

ä»å®éªŒç»“æœå¯ä»¥çœ‹å‡º, å½“ä½¿ç”¨ä½ç½®ç¼–ç æ—¶, æ¨¡å‹çš„ BLEU åˆ†æ•°é«˜äºä¸ä½¿ç”¨ä½ç½®ç¼–ç æ—¶çš„ BLEU åˆ†æ•°, ä¸”æå‡äº† 0.0591. ç”±æ­¤ä¹Ÿå¯ä»¥çœ‹å‡º, ä½ç½®ç¼–ç å¯¹äº Transformer æ¨¡å‹æ¥è¯´, æ˜¯ä¸€ä¸ªååˆ†é‡è¦çš„æ¨¡å—. 





## ğŸ’™ é¡¹ç›®å¿ƒå¾—

é€šè¿‡æœ¬æ¬¡å®éªŒï¼Œæˆ‘å¯¹ Transformer æ¨¡å‹çš„ç»“æ„ä¸åŸç†æœ‰äº†æ›´åŠ å…¨é¢ã€æ·±å…¥çš„è®¤è¯†ã€‚é¦–å…ˆï¼Œæˆ‘è¯¦ç»†å‰–æäº†å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆMultiâ€Head Selfâ€Attentionï¼‰ã€å‰é¦ˆç½‘ç»œï¼ˆFeedâ€Forward Networkï¼‰ä»¥åŠæ®‹å·®è¿æ¥ä¸å±‚å½’ä¸€åŒ–ï¼ˆResidual Connection & Layer Normalizationï¼‰ä¸‰å¤§æ ¸å¿ƒæ¨¡å—çš„å†…éƒ¨è¿ä½œåŸç†ï¼Œå¹¶é€šè¿‡ç»˜åˆ¶æ¨¡å‹ç»“æ„å›¾åŠ æ·±è®°å¿†ã€‚

åœ¨å®ç°ç¯èŠ‚ï¼Œæˆ‘ä½¿ç”¨ PyTorch ä»é›¶æ„å»ºäº†ä¸€ä¸ªç®€åŒ–ç‰ˆçš„ Transformer ç¼–ç å™¨â€“è§£ç å™¨æ¶æ„ï¼ŒåŒ…æ‹¬è¯åµŒå…¥ï¼ˆEmbeddingï¼‰ã€ä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰å’Œæ©ç æœºåˆ¶ï¼ˆMaskingï¼‰çš„å®Œæ•´æµæ°´çº¿ã€‚é€šè¿‡è°ƒè¯•å’Œå•å…ƒæµ‹è¯•ï¼Œæˆ‘æŒæ¡äº†å¦‚ä½•åœ¨ä»£ç å±‚é¢çµæ´»åœ°æ§åˆ¶æ³¨æ„åŠ›æƒé‡çš„è®¡ç®—ã€æ¢¯åº¦åå‘ä¼ æ’­ä¸å‚æ•°æ›´æ–°ã€‚

ä¸ºäº†éªŒè¯æ¨¡å‹åœ¨æœºå™¨ç¿»è¯‘ä»»åŠ¡ä¸Šçš„æ•ˆæœï¼Œæˆ‘é‡‡ç”¨äº†æ ‡å‡†çš„ BLEU åˆ†æ•°ä½œä¸ºå®šé‡è¯„ä¼°æŒ‡æ ‡ï¼Œå¹¶åœ¨ä¸­è‹±å¯¹ç…§è¯­æ–™é›†ä¸Šè¿›è¡Œäº†å¤šè½®å®éªŒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“è®­ç»ƒè½®æ¬¡è¾¾åˆ° 10 è½®ä»¥ä¸Šæ—¶ï¼Œæ¨¡å‹å°±å‡ºç°äº†è¿‡æ‹Ÿåˆç°è±¡ï¼ŒéªŒè¯é›†çš„ BLEU åˆ†æ•°å¼€å§‹ä¸‹é™ã€‚å› æ­¤ï¼Œæˆ‘é€‰æ‹©åœ¨ç¬¬ 10 è½®è®­ç»ƒç»“æŸåä¿å­˜æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨è¯¥æ¨¡å‹è¿›è¡Œç¿»è¯‘ä»»åŠ¡ã€‚

åœ¨æ‰‹åŠ¨æ£€éªŒç¯èŠ‚ï¼Œæˆ‘éšæœºé€‰å–äº† 3 ä¸ªæµ‹è¯•æ ·æœ¬ï¼Œå¹¶ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œç¿»è¯‘ã€‚é€šè¿‡å¯¹æ¯”æ ‡å‡†ç­”æ¡ˆä¸æ¨¡å‹ç¿»è¯‘ç»“æœï¼Œæˆ‘å‘ç°å°½ç®¡å­˜åœ¨ä¸€äº›ç»†èŠ‚ä¸Šçš„ä¸è¶³ï¼Œä½†æ•´ä½“ç¿»è¯‘æ•ˆæœè¿˜æ˜¯è¾ƒä¸ºä»¤äººæ»¡æ„çš„ã€‚

æ¶ˆèå®éªŒéƒ¨åˆ†, æˆ‘ç³»ç»Ÿåœ°æ¢ç©¶äº†æ³¨æ„åŠ›å¤´æ•°ã€ç¼–ç å™¨å’Œè§£ç å™¨å±‚æ•°å’Œä½ç½®ç¼–ç å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“. å…¶ä¸­, ä½ç½®ç¼–ç å¯¹ Transformer æ¨¡å‹çš„æ€§èƒ½æå¤§, è€Œæ³¨æ„åŠ›å¤´æ•°å’Œç¼–ç å™¨å’Œè§£ç å™¨å±‚æ•°åˆ™éœ€è¦æ ¹æ®å…·ä½“çš„ä»»åŠ¡å’Œæ•°æ®é›†çš„å¤§å°è¿›è¡Œè°ƒæ•´, é€‰æ‹©æœ€åˆé€‚çš„å‚æ•°. 

é€šè¿‡è¿™æ¬¡å®è·µï¼Œæˆ‘ä¸ä»…åŠ æ·±äº†å¯¹å‰æ²¿æ¨¡å‹çš„ç†è®ºç†è§£ï¼Œä¹Ÿåœ¨å·¥ç¨‹å®ç°ä¸å®éªŒè¯„ä¼°æ–¹é¢ç§¯ç´¯äº†å®è´µç»éªŒï¼Œä¸ºåç»­æ›´å¤æ‚çš„è‡ªç„¶è¯­è¨€å¤„ç†é¡¹ç›®æ‰“ä¸‹äº†åšå®åŸºç¡€ã€‚


## ğŸ“º æ¼”ç¤ºè§†é¢‘

[æ¼”ç¤ºè§†é¢‘](https://box.nju.edu.cn/f/e0f8854b0b0a47fbac4c/) ä¸­åŒ…å«äº†æ¨¡å‹çš„è®­ç»ƒå’Œè¯„ä¼°è¿‡ç¨‹.

## ğŸ“œ å‚è€ƒèµ„æ–™

[1] [Attention Is All You Need. NeurIPS 2017](https://arxiv.org/abs/1706.03762)

[2] [Yi-Fan Zhou's Blog](https://zhuanlan.zhihu.com/p/581334630)

[3] [æœºå™¨ç¿»è¯‘è¯„ä»·æŒ‡æ ‡BLEUä»‹ç»](https://blog.csdn.net/g11d111/article/details/100103208)

[4] [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)

[5] [Learning Deep Transformer Models for Machine Translation. ACL 2019](https://arxiv.org/abs/1906.01787)

[6] [ã€Šç¥ç»ç½‘ç»œä¸æ·±åº¦å­¦ä¹ ã€‹ é‚±é”¡é¹](https://nndl.github.io/)