# åŸºäº Transformer çš„æœºå™¨ç¿»è¯‘ï¼ˆè‹±è¯‘ä¸­ï¼‰

## ğŸ“‹ é¡¹ç›®èƒŒæ™¯


### 1. æœºå™¨ç¿»è¯‘çš„æŠ€æœ¯æ¼”è¿›
æœºå™¨ç¿»è¯‘ï¼ˆMachine Translation, MTï¼‰ä½œä¸ºè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸçš„æ ¸å¿ƒä»»åŠ¡ä¹‹ä¸€, ç»å†äº†ä¸‰ä¸ªä¸»è¦å‘å±•é˜¶æ®µ: 
- **è§„åˆ™é©±åŠ¨æ—¶ä»£**ï¼ˆ1950s-1990sï¼‰: åŸºäºè¯­è¨€å­¦ä¸“å®¶åˆ¶å®šçš„è¯­æ³•è§„åˆ™å’ŒåŒè¯­è¯å…¸è¿›è¡Œç›´è¯‘, å—é™äºè¯­è¨€å¤æ‚æ€§éš¾ä»¥å®ç°æµç•…ç¿»è¯‘
- **ç»Ÿè®¡å­¦ä¹ æ—¶ä»£**ï¼ˆ2000s-2010sï¼‰: IBMæå‡ºçš„åŸºäºçŸ­è¯­çš„ç»Ÿè®¡æœºå™¨ç¿»è¯‘ï¼ˆSMTï¼‰æˆä¸ºä¸»æµ, åˆ©ç”¨å¤§è§„æ¨¡åŒè¯­è¯­æ–™åº“å­¦ä¹ ç¿»è¯‘æ¦‚ç‡æ¨¡å‹
- **ç¥ç»ç½‘ç»œæ—¶ä»£**ï¼ˆ2017-è‡³ä»Šï¼‰: 2017 å¹´ Google æå‡ºçš„ Transformer æ¶æ„å¼•å‘é©å‘½, å…¶è‡ªæ³¨æ„åŠ›æœºåˆ¶çªç ´äº†ä¼ ç»ŸRNNçš„åºåˆ—å»ºæ¨¡ç“¶é¢ˆ

### 2. è‹±è¯‘ä¸­ä»»åŠ¡çš„ç‰¹æ®ŠæŒ‘æˆ˜
ä¸­æ–‡ä¸è‹±è¯­çš„è·¨è¯­ç§ç¿»è¯‘å­˜åœ¨å¤šé‡éš¾ç‚¹: 
- **ç»“æ„å·®å¼‚**: è‹±è¯­çš„ SVOCï¼ˆä¸»è°“å®¾è¡¥ï¼‰ç»“æ„ä¸ä¸­æ–‡çš„æ„åˆè¯­æ³•å­˜åœ¨æ˜ å°„é¸¿æ²Ÿ
- **è¯­ä¹‰é¸¿æ²Ÿ**: æˆè¯­ï¼ˆå¦‚"ç”»è›‡æ·»è¶³"ï¼‰ã€æ–‡åŒ–ä¸“æœ‰é¡¹ï¼ˆå¦‚"çº¢åŒ…"ï¼‰çš„ç­‰æ•ˆè¡¨è¾¾é—®é¢˜
- **æ•°æ®ç¨€ç¼ºæ€§**: é«˜è´¨é‡è‹±ä¸­å¹³è¡Œè¯­æ–™è§„æ¨¡ä»…ä¸ºè‹±æ³•åŒè¯­æ•°æ®çš„ 1/5ï¼ˆWMT 2020 ç»Ÿè®¡ï¼‰

### 3. Transformer çš„æŠ€æœ¯ä¼˜åŠ¿
æœ¬é¡¹ç›®é€‰ç”¨ Transformer æ¶æ„çš„æ ¸å¿ƒç†ç”±: 
| ç‰¹æ€§                | RNN/LSTM          | Transformer       |
|---------------------|-------------------|-------------------|
| é•¿è·ç¦»ä¾èµ–å»ºæ¨¡       | éšè·ç¦»è¡°å‡         | å…¨å±€æ³¨æ„åŠ›        |
| è®­ç»ƒå¹¶è¡Œåº¦           | åºåˆ—é€æ­¥è®¡ç®—       | å…¨åºåˆ—å¹¶è¡Œ        |
| è®¡ç®—å¤æ‚åº¦           | O(n)              | O(nÂ²)            |
| ä½ç½®æ•æ„Ÿæ€§           | å›ºæœ‰é¡ºåº          | éœ€ä½ç½®ç¼–ç         |


## ğŸ› ï¸ å®‰è£…

### ç¯å¢ƒé…ç½®

```bash
conda create -n translator python=3.10
conda activate translator
pip install -r requirements.txt
```

### æ•°æ®é›†é…ç½®

æœ¬é¡¹ç›®ä½¿ç”¨ [cmn-eng-simple](https://box.nju.edu.cn/d/b8245873f1e44c9fab65/) æ•°æ®é›†, åŒ…å«è‹±ä¸­å¹³è¡Œè¯­æ–™. é€šè¿‡é“¾æ¥ä¸‹è½½å, å°†å…¶æ”¾ç½®åœ¨ `data` ç›®å½•, ç»“æ„å¦‚ä¸‹: 

```
data/
â””â”€â”€ cmn-eng-simple/
    â”œâ”€â”€ training.txt       
    â”œâ”€â”€ validation.txt     
    â”œâ”€â”€ testing.txt       
    â”œâ”€â”€ int2word_cn.json   
    â”œâ”€â”€ word2int_cn.json  
    â”œâ”€â”€ int2word_en.json  
    â””â”€â”€ word2int_en.json  
```

## ğŸš€ å¿«é€Ÿè¿è¡Œ

``` bash
python main.py --period train
python main.py --period eval
```

## ğŸ“Š å®éªŒæŠ¥å‘Š

### æ¨¡å‹æ­å»º

Transformer æ¨¡å‹ç»“æ„å¦‚ä¸‹å›¾æ‰€ç¤º. å…¶ä¸­, æ¨¡å‹çš„ä¸»è¦æ¶æ„åŒ…æ‹¬å·¦ä¾§çš„ç¼–ç å™¨ï¼ˆEncoderï¼‰, ä»¥åŠå³ä¾§çš„è§£ç å™¨ï¼ˆDecoderï¼‰ä¸¤ä¸ªéƒ¨åˆ†. ç¼–ç å™¨å°†è¾“å…¥åºåˆ—è½¬æ¢ä¸ºä¸Šä¸‹æ–‡å‘é‡, è§£ç å™¨æ ¹æ®ä¸Šä¸‹æ–‡å‘é‡ç”Ÿæˆç›®æ ‡åºåˆ—. 

<img src="figs/transformer.png" height="600">

æˆ‘ä»¬å…ˆä»å°çš„ç»„ä»¶å¼€å§‹å®ç°, æœ€åå†å°†å®ƒä»¬ç»„åˆæˆå®Œæ•´çš„ Transformer æ¨¡å‹. 

1. input Embedding

    åœ¨å°†è‡ªç„¶è¯­è¨€è¾“å…¥æ¨¡å‹å‰, æˆ‘ä»¬é¦–å…ˆä¼šå¯¹å…¶è¿›è¡Œåˆ†è¯, å†æ ¹æ®è¯æ±‡è¡¨å°†æ¯ä¸ªè¯æ ¹è½¬æ¢ä¸ºå¯¹åº”çš„ token. ä¾‹å¦‚ `i am a student .` ä¼šè¢«è½¬åŒ–ä¸º `[5, 98, 9, 415, 4]`. è¿™æ ·è‡ªç„¶è¯­è¨€å°±å˜æˆäº†è®¡ç®—æœºå¯ä»¥ç†è§£çš„æ•°å€¼å½¢å¼. å½“ç„¶, å¯¹äºæ·±åº¦å­¦ä¹ æ¨¡å‹æ¥è¯´, è¿™è¿˜ä¸å¤Ÿ. ç°åœ¨æ¯ä¸ª token è¿˜æ˜¯å¤„äºæ–‡æœ¬ç©ºé—´å½“ä¸­, æˆ‘ä»¬å¸Œæœ›å°†å…¶æŠ•å½±åˆ°æ¨¡å‹çš„è¯­ä¹‰ç©ºé—´, ä»¥ä¾¿æ¨¡å‹æ›´å¥½åœ°ç†è§£å’Œå¤„ç†å…¶ç‰¹å¾. PyTorch å·²ç»æä¾›äº†ä¸€ä¸ªæ¨¡å— `torch.nn.Embedding` ç”¨äºè¯¥æ“ä½œ. åˆå§‹åŒ–æ—¶, ä¸æœºå™¨ç¿»è¯‘ç›¸å…³çš„å‚æ•°åŒ…æ‹¬

    - è¯æ±‡è¡¨å¤§å°: `num_embeddings (int)`
    - ç‰¹å¾ç»´åº¦: `embedding_dim (int)`
    - å¡«å……æ ‡è®°: `padding_idx (int, optional)`

    åœ¨æ„å»ºæœ€ç»ˆçš„æ¨¡å‹æ—¶, æˆ‘ä»¬ç›´æ¥ä½¿ç”¨å³å¯. 

2. Positional Encoding

    åœ¨è‡ªç„¶è¯­è¨€ä¸­, å•è¯çš„é¡ºåºæ˜¯éå¸¸é‡è¦çš„. ä¸ºäº†è®©æ¨¡å‹èƒ½å¤Ÿç†è§£å•è¯çš„é¡ºåº, æˆ‘ä»¬éœ€è¦ä¸ºæ¯ä¸ªå•è¯æ·»åŠ ä¸€ä¸ªä½ç½®ç¼–ç . ä½ç½®ç¼–ç æ˜¯ä¸€ä¸ªä¸å•è¯åµŒå…¥ï¼ˆword embeddingï¼‰ç›¸åŒç»´åº¦çš„å‘é‡, å®ƒåŒ…å«äº†å•è¯åœ¨å¥å­ä¸­çš„ä½ç½®ä¿¡æ¯. è®ºæ–‡ä¸­ç»™å‡ºçš„ä½ç½®ç¼–ç å…¬å¼å¦‚ä¸‹: 

    $PE_{(pos, 2i)} = sin(pos / 10000^{2i/d_{model}})$

    $PE_{(pos, 2i+1)} = cos(pos / 10000^{2i/d_{model}})$

    å…¶ä¸­, $pos$ æ˜¯å•è¯åœ¨å¥å­ä¸­çš„ä½ç½®, $i$ æ˜¯ä½ç½®ç¼–ç çš„ç»´åº¦ç´¢å¼•, $d_{model}$ æ˜¯åµŒå…¥å‘é‡çš„ç»´åº¦. æˆ‘ä»¬å¯ä»¥ç›´æ¥å°†å…¶ç†è§£ä¸ºä¸€ä¸ªç¼–ç å±‚, Transformer çš„è¾“å…¥é¦–å…ˆé€šè¿‡è¯¥ç¼–ç å±‚, è·å¾—ä½ç½®ç¼–ç . ä»£ç å®ç°å¦‚ä¸‹: 

    ```python
    class PositionEncoding(nn.Module):
        def __init__(self, max_seq_len: int, d_model: int):
            super().__init__()
            assert d_model % 2 == 0, "d_model must be even."
            # ç”±äºä½ç½®ç¼–ç æ˜¯ä¸€ä¸ªäºŒå…ƒå‡½æ•°, å¯ä»¥è€ƒè™‘ä½¿ç”¨ä¸€ä¸ªäºŒç»´çš„çŸ©é˜µæ¥è¡¨ç¤º
            i_pos = torch.linspace(
                0, max_seq_len - 1, max_seq_len
            )  # [0, 1, 2, ..., max_len-1] è¡¨ç¤º pos
            j_dim = torch.linspace(
                0, d_model - 2, d_model // 2
            )  # [0, 2, 4, ..., d_model-2] è¡¨ç¤ºå¶æ•°çš„ dim

            # ç”Ÿæˆä¸€ä¸ª [max_len, d_model//2] çš„ç½‘æ ¼ç½‘æ ¼
            # pos.shape: [max_len, d_model//2], two_i.shape: [max_len, d_model//2]
            pos, two_i = torch.meshgrid(i_pos, j_dim, indexing="ij")
            # pe_two_i.shape: [max_len, d_model//2]
            pe_two_i = torch.sin(pos / (10000 ** (two_i / d_model)))
            # pe_two_i_1.shape: [max_len, d_model//2]
            pe_two_i_1 = torch.cos(pos / (10000 ** (two_i / d_model)))

            # å°† pe_two_i å’Œ pe_two_i_1 æ‹¼æ¥æˆä¸€ä¸ª [max_len, d_model] çš„çŸ©é˜µ
            # è€ƒè™‘å…ˆæ‹¼æˆ [max_len, d_model//2, 2] çš„çŸ©é˜µ, ç„¶åå†å±•å¹³
            # åˆ©ç”¨ torch.stack å°†ä¸¤ä¸ª tensor æ²¿ç€æœ€åä¸€ä¸ªç»´åº¦å †å  (è‹¥ä½¿ç”¨ torch.cat çš„è¯åˆ™ç›´æ¥è¿æ¥äº†, ä¸ç¬¦åˆå¶å¥‡é—´éš”çš„è¦æ±‚)
            pe = torch.stack([pe_two_i, pe_two_i_1], dim=-1)  # [max_len, d_model//2, 2]
            pe = pe.reshape(
                1, max_seq_len, d_model
            )  # [1, max_len, d_model] é¢„ç•™å‡º batch ç»´åº¦
            # æ³¨å†Œä¸º buffer, å³ä¸éœ€è¦æ›´æ–°çš„å‚æ•° (éœ€è¦æ›´æ–°çš„å‚æ•°ä¸º parameter)
            # å½“æˆ‘ä»¬ä½¿ç”¨ model.to(device) æ—¶, buffer å’Œ parameter éƒ½ä¼šè‡ªåŠ¨è½¬ç§»åˆ°å¯¹åº”çš„è®¾å¤‡ä¸Š
            self.register_buffer("pe", pe, False)

        def forward(self, x: torch.Tensor) -> torch.Tensor:
            """
            Args:
                x: [batch_size, seq_len, d_model]
            Returns:
                x: [batch_size, seq_len, d_model]
            """
            _, seq_len, d_model = x.shape
            assert seq_len <= self.pe.shape[1], "seq_len exceeds max_len."
            assert d_model == self.pe.shape[2], "d_model mismatch."
            # ç›´æ¥åŠ ä¸Šä½ç½®ç¼–ç 
            return x + self.pe[:, :seq_len, :]
    ```

3. Multi-Head Attention

    åœ¨ Transformer ä¸­, æ³¨æ„åŠ›æœºåˆ¶æ˜¯ä¸€ä¸ªéå¸¸é‡è¦çš„ç»„æˆéƒ¨åˆ†. å®ƒå…è®¸æ¨¡å‹åœ¨å¤„ç†è¾“å…¥åºåˆ—æ—¶, å…³æ³¨åºåˆ—ä¸­çš„ä¸åŒéƒ¨åˆ†. Multi-Head Attention æ˜¯ä¸€ç§å°†å¤šä¸ªæ³¨æ„åŠ›å¤´ï¼ˆattention headï¼‰ç»“åˆèµ·æ¥çš„æ–¹æ³•. æ¯ä¸ªæ³¨æ„åŠ›å¤´éƒ½æœ‰è‡ªå·±çš„å‚æ•°é›†, å¯ä»¥å­¦ä¹ åˆ°ä¸åŒçš„ç‰¹å¾è¡¨ç¤º. æœ€ç»ˆçš„è¾“å‡ºæ˜¯æ‰€æœ‰æ³¨æ„åŠ›å¤´çš„è¾“å‡ºæ‹¼æ¥åœ¨ä¸€èµ·. 

    <img src="figs/attention.png" height="300">

    å…¶å…¬å¼å¦‚ä¸‹æ‰€ç¤º: 

    $\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W^O$

    å…¶ä¸­, $Q, K, V$ åˆ†åˆ«è¡¨ç¤ºæŸ¥è¯¢ï¼ˆQueryï¼‰ã€é”®ï¼ˆKeyï¼‰å’Œå€¼ï¼ˆValueï¼‰, $h$ è¡¨ç¤ºå¤´æ•°, $W^O$ æ˜¯ä¸€ä¸ªçº¿æ€§å˜æ¢çŸ©é˜µ. æ¯ä¸ªå¤´çš„è®¡ç®—å…¬å¼ä¸º: 

    $\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$

    å…¶ä¸­, $W_i^Q$, $W_i^K$, $W_i^V$ æ˜¯æ¯ä¸ªå¤´çš„çº¿æ€§å˜æ¢çŸ©é˜µ. æ³¨æ„åŠ›æœºåˆ¶çš„è®¡ç®—å…¬å¼ä¸º: 

    $\text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V$


    å…¶ä¸­, $d_k$ æ˜¯é”®çš„ç»´åº¦. 
    
    Multi-Head Attention çš„ä»£ç å®ç°å¦‚ä¸‹: 

    ```python
    class MultiHeadAttention(nn.Module):
        def __init__(self, d_model: int, heads: int, dropout: float = 0.1):
            super().__init__()

            assert d_model % heads == 0, "d_model must be divisible by heads."
            self.d_model = d_model
            self.heads = heads
            self.dim_head = d_model // heads
            inner_dim = heads * self.dim_head
            self.WQ = nn.Linear(d_model, inner_dim)
            self.WK = nn.Linear(d_model, inner_dim)
            self.WV = nn.Linear(d_model, inner_dim)
            self.attend = nn.Softmax(dim=-1)
            self.dropout = nn.Dropout(dropout)
            self.fc = nn.Linear(inner_dim, d_model)
            self.INF = float(1e12)

        def forward(
            self,
            q: torch.Tensor,
            k: torch.Tensor,
            v: torch.Tensor,
            mask: Optional[torch.Tensor] = None,
        ) -> torch.Tensor:
            """
            Args:
                q: [batch_size, q_len, d_model]
                k: [batch_size, k_len, d_model]
                v: [batch_size, k_len, d_model]
                mask (optional): [batch_size, 1, q_len, k_len]
            Returns:
                out: [batch_size, seq_len, d_model]
            """

            assert q.shape[0] == k.shape[0] == v.shape[0], "batch size mismatch."
            assert k.shape[1] == v.shape[1], "key and value length mismatch."
            # [batch_size, len, d_model] 
            # -> [batch_size, len, inner_dim] 
            # -> [batch_size, len, heads, dim_head] 
            # -> [batch_size, heads, len, dim_head]
            Q = rearrange(self.WQ(q), "b l (h d) -> b h l d", h=self.heads)
            K = rearrange(self.WK(k), "b l (h d) -> b h l d", h=self.heads)
            V = rearrange(self.WV(v), "b l (h d) -> b h l d", h=self.heads)
            dots = torch.matmul(Q, K.transpose(-2, -1)) / (
                self.dim_head**0.5
            )  # [batch_size, heads, q_len, k_len]
            if mask is not None:
                # mask ä¸ºåˆ¤æ–­æ¡ä»¶, å°† mask ä¸º True çš„éƒ¨åˆ†å¡«å……ä¸ºè‡ªå®šä¹‰çš„ -inf
                # å¦‚æœç›´æ¥ä½¿ç”¨ python è‡ªå¸¦çš„ inf è¿›è¡Œå¡«å……çš„è¯ä¼šå¾—åˆ° nan
                dots.masked_fill_(mask, -self.INF)
            attn = self.attend(dots)
            out = rearrange(
                torch.matmul(attn, V), "b h l d -> b l (h d)"
            )  # [batch_size, q_len, inner_dim]
            out = self.fc(self.dropout(out))
            return out
    ```
    > éœ€è¦ç‰¹åˆ«æ³¨æ„çš„æ˜¯, æ­¤å¤„åœ¨è®¾ç½® mask æ—¶, æˆ‘ä»¬æ²¡æœ‰ç›´æ¥ä½¿ç”¨ Python ä¸­è‡ªå¸¦çš„ inf, è€Œæ˜¯è®¾ç½®äº†ä¸€ä¸ªè‡ªå®šä¹‰çš„å¤§æ•°. è‹¥ç›´æ¥ä½¿ç”¨ inf, åœ¨è®¡ç®— Softmax å, ä¼šå‡ºç°å¯¹åº”ä½ç½®å€¼ä¸º nan çš„æƒ…å†µ. 

4. Feed Forward

    Feed Forward æ˜¯ Transformer ä¸­çš„ä¸€ä¸ªé‡è¦ç»„æˆéƒ¨åˆ†. å®ƒæ˜¯ä¸€ä¸ªä¸¤å±‚çš„å‰é¦ˆç¥ç»ç½‘ç»œ, é€šå¸¸ä½¿ç”¨ ReLU æ¿€æ´»å‡½æ•°. å…¶å…¬å¼å¦‚ä¸‹: 

    $\text{FFN}(x) = \text{ReLU}(xW_1 + b_1)W_2 + b_2$
    
    å…¶ä¸­, $W_1, b_1$ æ˜¯ç¬¬ä¸€å±‚çš„æƒé‡å’Œåç½®, $W_2, b_2$ æ˜¯ç¬¬äºŒå±‚çš„æƒé‡å’Œåç½®. Feed Forward çš„ä»£ç å®ç°å¦‚ä¸‹: 

    ```python
    class FeedForward(nn.Module):
        def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):
            super().__init__()
            self.linear1 = nn.Linear(d_model, d_ff)
            self.dropout = nn.Dropout(dropout)
            self.linear2 = nn.Linear(d_ff, d_model)
            self.relu = nn.ReLU()

        def forward(self, x: torch.Tensor) -> torch.Tensor:
            """
            Args:
                x: [batch_size, seq_len, d_model]
            Returns:
                out: [batch_size, seq_len, d_model]
            """
            out = self.linear1(x)
            out = self.relu(out)
            out = self.dropout(out)
            out = self.linear2(out)
            return out
    ```

5. Encoder Layer

    <img src="figs/encoder_layer.png" height="300">

    æ¯ä¸ª Encoder Layer ä¸»è¦ç”±ä¸¤éƒ¨åˆ†ç»„æˆ: Multi-Head Attentionï¼ˆæ­¤å¤„çš„ Multi-Head Attention ä¸º Self-Attentionï¼‰ å’Œ Feed Forward. å®ƒä»¬ä¹‹é—´è¿˜æœ‰ä¸€ä¸ª Add & Norm çš„å¤„ç†, å³æ®‹å·®è¿æ¥ï¼ˆResidual Connectionï¼‰å’Œå±‚å½’ä¸€åŒ–ï¼ˆLayer Normalizationï¼‰. æ®‹å·®è¿æ¥å…è®¸æ¢¯åº¦åœ¨åå‘ä¼ æ’­æ—¶æ›´å®¹æ˜“åœ°æµè¿‡ç½‘ç»œ, ä»è€ŒåŠ é€Ÿè®­ç»ƒ. å±‚å½’ä¸€åŒ–ç”¨äºç¨³å®šè®­ç»ƒè¿‡ç¨‹. Encoder Layer çš„ä»£ç å®ç°å¦‚ä¸‹: 

    ```python
    class EncoderLayer(nn.Module):
        def __init__(self, d_model: int, heads: int, d_ff: int, dropout: float = 0.1):
            super().__init__()
            self.self_attention = MultiHeadAttention(d_model, heads, dropout)
            self.ffn = FeedForward(d_model, d_ff, dropout)
            self.dropout1 = nn.Dropout(dropout)
            self.ln1 = nn.LayerNorm(d_model)
            self.dropout2 = nn.Dropout(dropout)
            self.ln2 = nn.LayerNorm(d_model)

        def forward(
            self, x: torch.Tensor, mask: Optional[torch.Tensor] = None
        ) -> torch.Tensor:
            x = self.ln1(x + self.dropout1(self.self_attention(x, x, x, mask)))
            x = self.ln2(x + self.dropout2(self.ffn(x)))
            return x
    ```

6. Decoder Layer

    Decoder Layer ä¸ Encoder Layer ç±»ä¼¼. ä½†éœ€è¦æ³¨æ„çš„æ˜¯, Decoder Layer ä¸­ç¬¬ä¸€ä¸ª Multi-Head Attention ä¸º Self-Attention, å…¶ä¸­, Qã€Kã€V å‡æ¥è‡ª Decoder çš„è¾“å…¥ï¼›ç¬¬äºŒä¸ª Multi-Head Attention åˆ™ä¸º Cross-Attention, å…¶ä¸­, Q æ¥è‡ª Decoder çš„è¾“å…¥, Kã€V åˆ™ä¸º Encoder çš„è¾“å‡º. å…¶ä»£ç å®ç°å¦‚ä¸‹: 

    ```python
    class DecoderLayer(nn.Module):
        def __init__(self, d_model: int, heads: int, d_ff: int, dropout: float = 0.1):
            super().__init__()
            self.self_attention = MultiHeadAttention(d_model, heads, dropout)
            self.dropout1 = nn.Dropout(dropout)
            self.ln1 = nn.LayerNorm(d_model)
            self.cross_attention = MultiHeadAttention(d_model, heads, dropout)
            self.dropout2 = nn.Dropout(dropout)
            self.ln2 = nn.LayerNorm(d_model)
            self.ffn = FeedForward(d_model, d_ff, dropout)
            self.dropout3 = nn.Dropout(dropout)
            self.ln3 = nn.LayerNorm(d_model)

        def forward(
            self,
            x: torch.Tensor,
            encoder_kv: torch.Tensor,
            dst_mask: Optional[torch.Tensor] = None,
            src_dst_mask: Optional[torch.Tensor] = None,
        ) -> torch.Tensor:
            x = self.ln1(x + self.dropout1(self.self_attention(x, x, x, dst_mask)))
            x = self.ln2(
                x
                + self.dropout2(
                    self.cross_attention(x, encoder_kv, encoder_kv, src_dst_mask)
                )
            )
            x = self.ln3(x + self.dropout3(self.ffn(x)))
            return x
    ```

7. Encoder

    ç¼–ç å™¨ç”±å¤šä¸ªç¼–ç å™¨å±‚å †å è€Œæˆ. å…¶ä»£ç å®ç°å¦‚ä¸‹: 

    ```python
    class Encoder(nn.Module):
        def __init__(
            self,
            vocab_size: int,
            pad_idx: int,
            d_model: int,
            d_ff: int,
            n_layers: int,
            heads: int,
            dropout: float = 0.1,
            max_seq_len: int = 512,
        ):
            super().__init__()
            # pad ä¸å‚ä¸æ¢¯åº¦è®¡ç®—
            self.embedding = nn.Embedding(vocab_size, d_model, pad_idx)
            self.position_encoding = PositionEncoding(max_seq_len, d_model)
            self.layers = nn.ModuleList(
                [EncoderLayer(d_model, heads, d_ff, dropout) for _ in range(n_layers)]
            )
            self.dropout = nn.Dropout(dropout)

        def forward(self, x, src_mask: Optional[torch.Tensor] = None):
            x = self.embedding(x)
            x = self.position_encoding(x)
            x = self.dropout(x)
            for layer in self.layers:
                x = layer(x, src_mask)
            return x
    ```

8. Decoder

    è§£ç å™¨ç”±å¤šä¸ªè§£ç å™¨å±‚å †å è€Œæˆ. å…¶ä»£ç å®ç°å¦‚ä¸‹: 

    ```python
    class Decoder(nn.Module):
        def __init__(
            self,
            vocab_size: int,
            pad_idx: int,
            d_model: int,
            d_ff: int,
            n_layers: int,
            heads: int,
            dropout: float = 0.1,
            max_seq_len: int = 512,
        ):
            super().__init__()
            self.embedding = nn.Embedding(vocab_size, d_model, pad_idx)
            self.position_encoding = PositionEncoding(max_seq_len, d_model)
            self.layers = nn.ModuleList(
                [DecoderLayer(d_model, heads, d_ff, dropout) for _ in range(n_layers)]
            )
            self.dropout = nn.Dropout(dropout)

        def forward(
            self,
            x: torch.Tensor,
            encoder_kv,
            dst_mask: Optional[torch.Tensor] = None,
            src_dst_mask: Optional[torch.Tensor] = None,
        ):
            x = self.embedding(x)
            x = self.position_encoding(x)
            x = self.dropout(x)
            for layer in self.layers:
                x = layer(x, encoder_kv, dst_mask, src_dst_mask)
            return x
    ```

æœ‰äº†ä»¥ä¸Šçš„ç»„ä»¶, æˆ‘ä»¬å°±å¯ä»¥å°†å®ƒä»¬ç»„åˆæˆå®Œæ•´çš„ Transformer æ¨¡å‹äº†. Transformer æ¨¡å‹çš„ä»£ç å®ç°å¦‚ä¸‹: 

```python
class Transformer(nn.Module):
    def __init__(
        self,
        src_vocab_size: int,
        dst_vocab_size: int,
        pad_idx: int,
        d_model: int,
        d_ff: int,
        n_layers: int,
        heads: int,
        dropout: float = 0.1,
        max_seq_len: int = 512,
    ):
        super().__init__()
        self.encoder = Encoder(
            src_vocab_size,
            pad_idx,
            d_model,
            d_ff,
            n_layers,
            heads,
            dropout,
            max_seq_len,
        )
        self.decoder = Decoder(
            dst_vocab_size,
            pad_idx,
            d_model,
            d_ff,
            n_layers,
            heads,
            dropout,
            max_seq_len,
        )
        self.pad_idx = pad_idx
        self.output_layer = nn.Linear(d_model, dst_vocab_size)

    def generate_mask(
        self, q_pad: torch.Tensor, k_pad: torch.Tensor, apply_causal_mask: bool = False
    ):
        # q_pad shape: [n, q_len]
        # k_pad shape: [n, k_len]
        # q_pad k_pad dtype: bool
        assert q_pad.device == k_pad.device, "padding mask must be same device."
        n, q_len = q_pad.shape
        n, k_len = k_pad.shape

        mask_shape = (n, 1, q_len, k_len)
        if apply_causal_mask:
            # Decoder mask
            mask = 1 - torch.tril(
                torch.ones(mask_shape)
            )  # å¯¹è§’çº¿ä»¥ä¸Šå…¨ä¸º 1, å³å±è”½ä¹‹å‰çš„ä¿¡æ¯

        else:
            # Encoder mask
            mask = torch.zeros(mask_shape)
        mask = mask.to(q_pad.device)
        for i in range(n):
            mask[i, :, q_pad[i], :] = 1
            mask[i, :, :, k_pad[i]] = 1
        mask = mask.to(torch.bool)
        return mask

    def forward(self, x, y):
        src_pad_mask = x == self.pad_idx
        dst_pad_mask = y == self.pad_idx
        src_mask = self.generate_mask(
            q_pad=src_pad_mask, k_pad=src_pad_mask, apply_causal_mask=False
        )  # ä½¿ç”¨ encoder mask
        dst_mask = self.generate_mask(
            q_pad=dst_pad_mask, k_pad=dst_pad_mask, apply_causal_mask=True
        )  # ä½¿ç”¨ decoder mask
        src_dst_mask = self.generate_mask(
            q_pad=dst_pad_mask, k_pad=src_pad_mask, apply_causal_mask=False
        )
        encoder_kv = self.encoder(x, src_mask)
        res = self.decoder(y, encoder_kv, dst_mask, src_dst_mask)
        res = self.output_layer(res)
        return res
```

é™¤äº†å·²æœ‰çš„å°ç»„ä»¶ä¹‹å¤–, å¯ä»¥çœ‹åˆ° Transformer ä¸­è¿˜åŒ…æ‹¬äº†ä¸€ä¸ªå¾ˆé‡è¦çš„å‡½æ•° `generate_mask`, è¯¥å‡½æ•°ç”¨äºç”Ÿæˆ padding mask å’Œ causal mask. padding mask ç”¨äºå±è”½æ‰è¾“å…¥åºåˆ—ä¸­çš„å¡«å……éƒ¨åˆ†, è€Œ causal mask åˆ™ç”¨äºå±è”½æ‰è§£ç å™¨ä¸­å½“å‰ token ä¹‹åçš„éƒ¨åˆ†. è¿™æ ·å¯ä»¥ç¡®ä¿æ¨¡å‹åœ¨ç”Ÿæˆä¸‹ä¸€ä¸ª token æ—¶, åªèƒ½çœ‹åˆ°å½“å‰ token ä¹‹å‰çš„éƒ¨åˆ†. 

è‡³æ­¤, æ¨¡å‹å°±å·²æ­å»ºå®Œæ¯•. 

### æ•°æ®å¤„ç†

å¦‚ `input embedding` ä¸­æåˆ°çš„, æˆ‘ä»¬éœ€è¦å°†è‡ªç„¶è¯­è¨€è½¬æ¢ä¸ºè®¡ç®—æœºå¯ä»¥ç†è§£çš„æ•°å€¼å½¢å¼. åœ¨ `cmn-eng-simple` æ•°æ®é›†å½“ä¸­, å·²ç»é¢„å…ˆå®šä¹‰å¥½äº†è‡ªç„¶è¯­è¨€è¯è¯­ä¸ token ä¹‹é—´çš„ä¸€å¯¹ä¸€æ˜ å°„å…³ç³», å…·ä½“å¯è§ `int2word_cn.json`ã€`int2word_en.json`ã€`word2int_cn.json`ã€`word2int_en.json` å››ä¸ªæ–‡ä»¶, æˆ‘ä»¬å¯ä»¥ç›´æ¥ä½¿ç”¨è¿™äº›æ–‡ä»¶æ¥è¿›è¡Œæ•°æ®å¤„ç†. ä¸ºäº†æ–¹ä¾¿åç»­çš„è®­ç»ƒå’Œè¯„æµ‹æ—¶æŒ‰ç…§ batch è¿›è¡Œå¤„ç†, æˆ‘ä»¬åŒæ—¶å°†æ•°æ®é›†å°è£…æˆäº† `torch.utils.data.Dataset` çš„å½¢å¼. æ•°æ®é›†çš„ä»£ç å®ç°å¦‚ä¸‹: 

```python
import os
import json
import torch
from torch.utils.data import Dataset

SPLIT = {"train": "training", "val": "validation", "test": "testing"}


class TranslationDataset(Dataset):
    def __init__(self, data_dir, split="train"):
        assert split in SPLIT.keys(), "Invalid split name."
        split = SPLIT[split]
        self.data_dir = data_dir
        self.int2cn, self.int2en, self.cn2int, self.en2int = self._read_vocab()
        data_file = os.path.join(data_dir, f"{split}.txt")
        if not os.path.exists(data_file):
            raise FileNotFoundError(f"Data file {data_file} not found.")
        self.pairs = []  # [([...], [...]), (...), ...]
        with open(data_file, "r", encoding="utf-8") as f:
            for line in f:
                en, cn = line.strip().split("\t")  # åˆ¶è¡¨ç¬¦ä½œä¸ºåˆ†éš”ç¬¦
                # å¤„ç†è‹±æ–‡ä¸­çš„@@ç¬¦å·
                en = en.replace("@@", "").split()  # é»˜è®¤åˆ†éš”ç¬¦æ˜¯ç©ºæ ¼
                cn = cn.split()
                self.pairs.append((en, cn))

    def __len__(self):
        return len(self.pairs)

    def __getitem__(self, idx):
        en_tokens = [
            self.en2int.get(tok, self.en2int["<UNK>"]) for tok in self.pairs[idx][0]
        ]
        cn_tokens = [
            self.cn2int.get(tok, self.cn2int["<UNK>"]) for tok in self.pairs[idx][1]
        ]

        # æ·»åŠ ç‰¹æ®Šæ ‡è®°
        en = [self.en2int["<BOS>"]] + en_tokens + [self.en2int["<EOS>"]]
        cn = [self.cn2int["<BOS>"]] + cn_tokens + [self.cn2int["<EOS>"]]

        return torch.LongTensor(en), torch.LongTensor(cn)

    def _read_vocab(self):
        data_dir = self.data_dir
        int2cn_file = os.path.join(data_dir, "int2word_cn.json")
        int2en_file = os.path.join(data_dir, "int2word_en.json")
        cn2int_file = os.path.join(data_dir, "word2int_cn.json")
        en2int_file = os.path.join(data_dir, "word2int_en.json")
        # åˆ¤æ–­æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if (
            not os.path.exists(int2cn_file)
            or not os.path.exists(int2en_file)
            or not os.path.exists(cn2int_file)
            or not os.path.exists(en2int_file)
        ):
            raise FileNotFoundError(
                "Vocabulary files not found in the specified directory."
            )
        with open(int2cn_file, "r", encoding="utf-8") as f:
            int2cn = json.load(f)
        with open(int2en_file, "r", encoding="utf-8") as f:
            int2en = json.load(f)
        with open(cn2int_file, "r", encoding="utf-8") as f:
            cn2int = json.load(f)
        with open(en2int_file, "r", encoding="utf-8") as f:
            en2int = json.load(f)

        return int2cn, int2en, cn2int, en2int
```

å…¶ä¸­, `<BOS>`ã€`<EOS>` å’Œ `<UNK>` åˆ†åˆ«è¡¨ç¤ºå¥å­çš„å¼€å§‹ã€ç»“æŸå’ŒæœªçŸ¥ token. æˆ‘ä»¬å¯ä»¥é€šè¿‡å¦‚ä¸‹æ–¹å¼æ£€éªŒèƒ½å¦æ­£ç¡®åŠ è½½æ•°æ®é›†: 

```python
if __name__ == "__main__":
    data_dir = "data/cmn-eng-simple"
    dataset = TranslationDataset(data_dir, split="train")
    print(f"Number of samples: {len(dataset)}\n")
    for i in range(3):
        en, cn = dataset[i]
        print("English: ", end=" ")
        for word in en:
            print(dataset.int2en[str(word.item())], end=" ")
        print("\nChinese: ", end=" ")
        for word in cn:
            print(dataset.int2cn[str(word.item())], end=" ")
        print("\n")
```

è¾“å‡ºç»“æœå¦‚ä¸‹æ‰€ç¤º: 

```txt
Number of samples: 18000

English:  <BOS> it 's none of your concern . <EOS> 
Chinese:  <BOS> è¿™ä¸å…³ ä½  çš„ äº‹ .  <EOS> 

English:  <BOS> she has a habit of <UNK> ting her na ils . <EOS> 
Chinese:  <BOS> å¥¹ æœ‰ å’¬ <UNK> çš„ ä¹ æƒ¯ .  <EOS> 

English:  <BOS> he is a teacher . <EOS> 
Chinese:  <BOS> ä»– æ˜¯ è€å¸ˆ .  <EOS>  
```

### æ¨¡å‹è®­ç»ƒåŠè¯„ä¼°

1. å®šä¹‰è¶…å‚æ•°

    ```python
    # Config
    period = args.period  # train or eval
    d_model = 512 # æ¨¡å‹å†…éƒ¨ç»´åº¦
    d_ff = 2048 # å‰é¦ˆç½‘ç»œç»´åº¦
    n_layers = 6 # ç¼–ç å™¨å’Œè§£ç å™¨å±‚æ•°
    heads = 8 # æ³¨æ„åŠ›å¤´æ•°
    dropout = 0.1 # dropout æ¦‚ç‡
    max_seq_len = 100 # æœ€å¤§åºåˆ—é•¿åº¦
    batch_size = 64 # æ‰¹æ¬¡å¤§å°
    lr = 1e-4 # å­¦ä¹ ç‡
    n_epochs = 60 # è®­ç»ƒè½®æ•°
    print_interval = 50 # æ‰“å°é—´éš” 
    device = "cuda" if torch.cuda.is_available() else "cpu" # è®¾å¤‡
    ```

    å…¶ä¸­ `period` é€šè¿‡å‘½ä»¤è¡Œå‚æ•°ä¼ å…¥, è¡¨ç¤ºå½“å‰æ˜¯è®­ç»ƒè¿˜æ˜¯éªŒè¯é˜¶æ®µ. 

2. åŠ è½½æ•°æ®é›†

    ```python
    data_dir = "data/cmn-eng-simple"
    train_set = TranslationDataset(data_dir, split="train")
    val_set = TranslationDataset(data_dir, split="val")
    test_set = TranslationDataset(data_dir, split="test")
    train_loader = build_dataloader(train_set, batch_size=batch_size)
    val_loader = build_dataloader(val_set, batch_size=batch_size)
    test_loader = build_dataloader(test_set, batch_size=batch_size)
    en2int, cn2int, int2en, int2cn = (
        train_set.en2int,
        train_set.cn2int,
        train_set.int2en,
        train_set.int2cn,
    )
    en_vocab_size = len(en2int)
    cn_vocab_size = len(cn2int)
    PAD_ID = en2int["<PAD>"]
    BOS_ID = cn2int["<BOS>"]
    ```

    å…¶ä¸­,  `build_dataloader` å‡½æ•°ç”¨äºæ„å»ºæ•°æ®åŠ è½½å™¨, ä»£ç å®ç°å¦‚ä¸‹: 

    ```python
    def build_dataloader(dataset, batch_size=32, shuffle=True, num_workers=4):
        en2int, cn2int = dataset.en2int, dataset.cn2int

        def _collate_fn(batch):
            en_batch, cn_batch = zip(*batch)

            # åŠ ä¸Š padding, è¡¥é½åˆ°ç›¸åŒé•¿åº¦, é»˜è®¤æ˜¯åœ¨å³ä¾§è¿›è¡Œ padding
            en_padded = pad_sequence(
                en_batch, batch_first=True, padding_value=en2int["<PAD>"]
            )

            cn_padded = pad_sequence(
                cn_batch, batch_first=True, padding_value=cn2int["<PAD>"]
            )
            return {
                "source": en_padded,
                "target": cn_padded,
            }

        return DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=shuffle,
            num_workers=num_workers,
            collate_fn=_collate_fn,
        )
    ```

    `_collate_fn` å‡½æ•°ååˆ†å…³é”®, ç”±äºæˆ‘ä»¬éœ€è¦ä»¥çŸ©é˜µçš„å½¢å¼å°†æ•°æ®è¾“å…¥æ¨¡å‹, æ‰€ä»¥æˆ‘ä»¬éœ€è¦ä¿è¯æ¯ä¸ª batch ä¸­çš„å¥å­é•¿åº¦ä¸€è‡´. æ•…è€ƒè™‘ä½¿ç”¨ `torch.nn.utils.rnn.pad_sequence` å‡½æ•°å¯¹æ¯ä¸ª batch ä¸­çš„å¥å­è¿›è¡Œ padding, è¡¥é½åˆ°ç›¸åŒé•¿åº¦. é»˜è®¤æƒ…å†µä¸‹, padding æ˜¯åœ¨å³ä¾§è¿›è¡Œçš„. 

3. å®šä¹‰æ¨¡å‹ 

    ç”±äºæˆ‘ä»¬åœ¨æ¨¡å‹æ­å»ºæ—¶å·²ç»å®šä¹‰å¥½äº†æ¨¡å‹çš„å„ä¸ªç»„ä»¶, æ‰€ä»¥åœ¨è¿™é‡Œæˆ‘ä»¬åªéœ€è¦å®ä¾‹åŒ–æ¨¡å‹å³å¯. ä»£ç å®ç°å¦‚ä¸‹: 

    ```python
    model = Transformer(
        src_vocab_size=en_vocab_size,
        dst_vocab_size=cn_vocab_size,
        pad_idx=PAD_ID,
        d_model=d_model,
        d_ff=d_ff,
        n_layers=n_layers,
        heads=heads,
        dropout=dropout,
        max_seq_len=max_seq_len,
    ).to(device)
    ```

    åœ¨å®šä¹‰å®Œæ¨¡å‹å, æˆ‘ä»¬å¯ä»¥é€šè¿‡è‡ªå®šä¹‰çš„å‡½æ•°æŸ¥çœ‹ä¸€ä¸‹æ¨¡å‹çš„ç»“æ„ä»¥åŠå‚æ•°é‡: 
    
    ```python
    def print_model_summary(model, depth=3):
    header = ["Layer (type)", "Output Shape", "Param #", "Trainable"]
    rows = []
    total_params = 0
    trainable_params = 0
    non_trainable_params = 0

    # é€’å½’éå†æ¨¡å‹ç»“æ„
    def _add_layer_info(module, name, depth):
        nonlocal total_params, trainable_params, non_trainable_params
        params = sum(np.prod(p.size()) for p in module.parameters())
        if params == 0:
            return

        # å‚æ•°ç»Ÿè®¡
        trainable = any(p.requires_grad for p in module.parameters())
        total_params += params
        if trainable:
            trainable_params += params
        else:
            non_trainable_params += params

        # æ„é€ è¾“å‡ºå½¢çŠ¶ï¼ˆç¤ºä¾‹ï¼‰
        output_shape = (
            "x".join(str(s) for s in module.example_output_shape)
            if hasattr(module, "example_output_shape")
            else "--"
        )

        # æ·»åŠ åˆ°è¡¨æ ¼
        rows.append(
            [
                name + f" ({module.__class__.__name__})",
                f"[{output_shape}]",
                f"{params:,}",
                "Yes" if trainable else "No",
            ]
        )

        # é€’å½’å­æ¨¡å—
        if depth > 0:
            for child_name, child_module in module.named_children():
                _add_layer_info(child_module, f"{name}.{child_name}", depth - 1)

    # éå†é¡¶å±‚æ¨¡å—
    for name, module in model.named_children():
        _add_layer_info(module, name, depth)

    # æ‰“å°è¡¨æ ¼
    from tabulate import tabulate

    print(tabulate(rows, headers=header, tablefmt="psql"))

    # å‚æ•°å•ä½è½¬æ¢
    def _format_num(num):
        if num >= 1e6:
            return f"{num/1e6:.2f}M"
        elif num >= 1e3:
            return f"{num/1e3:.1f}K"
        return str(num)

    # æ‰“å°æ±‡æ€»ä¿¡æ¯
    print(f"\n{'='*60}")
    print(f"Total params: {_format_num(total_params)} ({total_params:,})")
    print(f"Trainable params: {_format_num(trainable_params)} ({trainable_params:,})")
    print(
        f"Non-trainable params: {_format_num(non_trainable_params)} ({non_trainable_params:,})"
    )
    print(f"Model size: {total_params*4/(1024**2):.2f}MB (FP32)")  # å‡è®¾32ä½æµ®ç‚¹
    print("=" * 60 + "\n")
    ```

    è¾“å‡ºç»“æœå¦‚ä¸‹: 

    ```txt
    +-------------------------------------------------------+----------------+------------+-------------+
    | Layer (type)                                          | Output Shape   | Param #    | Trainable   |
    |-------------------------------------------------------+----------------+------------+-------------|
    | encoder (Encoder)                                     | [--]           | 20,922,368 | Yes         |
    | encoder.embedding (Embedding)                         | [--]           | 2,008,064  | Yes         |
    | encoder.layers (ModuleList)                           | [--]           | 18,914,304 | Yes         |
    | encoder.layers.0 (EncoderLayer)                       | [--]           | 3,152,384  | Yes         |
    | encoder.layers.0.self_attention (MultiHeadAttention)  | [--]           | 1,050,624  | Yes         |
    | encoder.layers.0.ffn (FeedForward)                    | [--]           | 2,099,712  | Yes         |
    | encoder.layers.0.ln1 (LayerNorm)                      | [--]           | 1,024      | Yes         |
    | encoder.layers.0.ln2 (LayerNorm)                      | [--]           | 1,024      | Yes         |
    | encoder.layers.1 (EncoderLayer)                       | [--]           | 3,152,384  | Yes         |
    | encoder.layers.1.self_attention (MultiHeadAttention)  | [--]           | 1,050,624  | Yes         |
    | encoder.layers.1.ffn (FeedForward)                    | [--]           | 2,099,712  | Yes         |
    | encoder.layers.1.ln1 (LayerNorm)                      | [--]           | 1,024      | Yes         |
    | encoder.layers.1.ln2 (LayerNorm)                      | [--]           | 1,024      | Yes         |
    | encoder.layers.2 (EncoderLayer)                       | [--]           | 3,152,384  | Yes         |
    | encoder.layers.2.self_attention (MultiHeadAttention)  | [--]           | 1,050,624  | Yes         |
    | encoder.layers.2.ffn (FeedForward)                    | [--]           | 2,099,712  | Yes         |
    | encoder.layers.2.ln1 (LayerNorm)                      | [--]           | 1,024      | Yes         |
    | encoder.layers.2.ln2 (LayerNorm)                      | [--]           | 1,024      | Yes         |
    | encoder.layers.3 (EncoderLayer)                       | [--]           | 3,152,384  | Yes         |
    | encoder.layers.3.self_attention (MultiHeadAttention)  | [--]           | 1,050,624  | Yes         |
    | encoder.layers.3.ffn (FeedForward)                    | [--]           | 2,099,712  | Yes         |
    | encoder.layers.3.ln1 (LayerNorm)                      | [--]           | 1,024      | Yes         |
    | encoder.layers.3.ln2 (LayerNorm)                      | [--]           | 1,024      | Yes         |
    | encoder.layers.4 (EncoderLayer)                       | [--]           | 3,152,384  | Yes         |
    | encoder.layers.4.self_attention (MultiHeadAttention)  | [--]           | 1,050,624  | Yes         |
    | encoder.layers.4.ffn (FeedForward)                    | [--]           | 2,099,712  | Yes         |
    | encoder.layers.4.ln1 (LayerNorm)                      | [--]           | 1,024      | Yes         |
    | encoder.layers.4.ln2 (LayerNorm)                      | [--]           | 1,024      | Yes         |
    | encoder.layers.5 (EncoderLayer)                       | [--]           | 3,152,384  | Yes         |
    | encoder.layers.5.self_attention (MultiHeadAttention)  | [--]           | 1,050,624  | Yes         |
    | encoder.layers.5.ffn (FeedForward)                    | [--]           | 2,099,712  | Yes         |
    | encoder.layers.5.ln1 (LayerNorm)                      | [--]           | 1,024      | Yes         |
    | encoder.layers.5.ln2 (LayerNorm)                      | [--]           | 1,024      | Yes         |
    | decoder (Decoder)                                     | [--]           | 27,156,992 | Yes         |
    | decoder.embedding (Embedding)                         | [--]           | 1,932,800  | Yes         |
    | decoder.layers (ModuleList)                           | [--]           | 25,224,192 | Yes         |
    | decoder.layers.0 (DecoderLayer)                       | [--]           | 4,204,032  | Yes         |
    | decoder.layers.0.self_attention (MultiHeadAttention)  | [--]           | 1,050,624  | Yes         |
    | decoder.layers.0.ln1 (LayerNorm)                      | [--]           | 1,024      | Yes         |
    | decoder.layers.0.cross_attention (MultiHeadAttention) | [--]           | 1,050,624  | Yes         |
    | decoder.layers.0.ln2 (LayerNorm)                      | [--]           | 1,024      | Yes         |
    | decoder.layers.0.ffn (FeedForward)                    | [--]           | 2,099,712  | Yes         |
    | decoder.layers.0.ln3 (LayerNorm)                      | [--]           | 1,024      | Yes         |
    | decoder.layers.1 (DecoderLayer)                       | [--]           | 4,204,032  | Yes         |
    | decoder.layers.1.self_attention (MultiHeadAttention)  | [--]           | 1,050,624  | Yes         |
    | decoder.layers.1.ln1 (LayerNorm)                      | [--]           | 1,024      | Yes         |
    | decoder.layers.1.cross_attention (MultiHeadAttention) | [--]           | 1,050,624  | Yes         |
    | decoder.layers.1.ln2 (LayerNorm)                      | [--]           | 1,024      | Yes         |
    | decoder.layers.1.ffn (FeedForward)                    | [--]           | 2,099,712  | Yes         |
    | decoder.layers.1.ln3 (LayerNorm)                      | [--]           | 1,024      | Yes         |
    | decoder.layers.2 (DecoderLayer)                       | [--]           | 4,204,032  | Yes         |
    | decoder.layers.2.self_attention (MultiHeadAttention)  | [--]           | 1,050,624  | Yes         |
    | decoder.layers.2.ln1 (LayerNorm)                      | [--]           | 1,024      | Yes         |
    | decoder.layers.2.cross_attention (MultiHeadAttention) | [--]           | 1,050,624  | Yes         |
    | decoder.layers.2.ln2 (LayerNorm)                      | [--]           | 1,024      | Yes         |
    | decoder.layers.2.ffn (FeedForward)                    | [--]           | 2,099,712  | Yes         |
    | decoder.layers.2.ln3 (LayerNorm)                      | [--]           | 1,024      | Yes         |
    | decoder.layers.3 (DecoderLayer)                       | [--]           | 4,204,032  | Yes         |
    | decoder.layers.3.self_attention (MultiHeadAttention)  | [--]           | 1,050,624  | Yes         |
    | decoder.layers.3.ln1 (LayerNorm)                      | [--]           | 1,024      | Yes         |
    | decoder.layers.3.cross_attention (MultiHeadAttention) | [--]           | 1,050,624  | Yes         |
    | decoder.layers.3.ln2 (LayerNorm)                      | [--]           | 1,024      | Yes         |
    | decoder.layers.3.ffn (FeedForward)                    | [--]           | 2,099,712  | Yes         |
    | decoder.layers.3.ln3 (LayerNorm)                      | [--]           | 1,024      | Yes         |
    | decoder.layers.4 (DecoderLayer)                       | [--]           | 4,204,032  | Yes         |
    | decoder.layers.4.self_attention (MultiHeadAttention)  | [--]           | 1,050,624  | Yes         |
    | decoder.layers.4.ln1 (LayerNorm)                      | [--]           | 1,024      | Yes         |
    | decoder.layers.4.cross_attention (MultiHeadAttention) | [--]           | 1,050,624  | Yes         |
    | decoder.layers.4.ln2 (LayerNorm)                      | [--]           | 1,024      | Yes         |
    | decoder.layers.4.ffn (FeedForward)                    | [--]           | 2,099,712  | Yes         |
    | decoder.layers.4.ln3 (LayerNorm)                      | [--]           | 1,024      | Yes         |
    | decoder.layers.5 (DecoderLayer)                       | [--]           | 4,204,032  | Yes         |
    | decoder.layers.5.self_attention (MultiHeadAttention)  | [--]           | 1,050,624  | Yes         |
    | decoder.layers.5.ln1 (LayerNorm)                      | [--]           | 1,024      | Yes         |
    | decoder.layers.5.cross_attention (MultiHeadAttention) | [--]           | 1,050,624  | Yes         |
    | decoder.layers.5.ln2 (LayerNorm)                      | [--]           | 1,024      | Yes         |
    | decoder.layers.5.ffn (FeedForward)                    | [--]           | 2,099,712  | Yes         |
    | decoder.layers.5.ln3 (LayerNorm)                      | [--]           | 1,024      | Yes         |
    | output_layer (Linear)                                 | [--]           | 1,936,575  | Yes         |
    +-------------------------------------------------------+----------------+------------+-------------+

    ============================================================
    Total params: 186.37M (186,372,287)
    Trainable params: 186.37M (186,372,287)
    Non-trainable params: 0 (0)
    Model size: 710.95MB (FP32)
    ============================================================
    ```

    å¯ä»¥çœ‹åˆ°æ¨¡å‹çš„å‚æ•°é‡å¤§çº¦ä¸º 1.86 äº¿, æ¨¡å‹å¤§å°å¤§çº¦ä¸º 710MB. 

4. æ¨¡å‹è®­ç»ƒ

    è¯¥éƒ¨åˆ†çš„ä»£ç ä¸å¸¸è§„æ·±åº¦å­¦ä¹ è®­ç»ƒè¿‡ç¨‹ç±»ä¼¼, æ•…ä¸å†èµ˜è¿°, éœ€è¦æ³¨æ„çš„ç»†èŠ‚å¯è§æ³¨é‡Š, ä»£ç å¦‚ä¸‹: 

    ```python
    if period == "train":
        optimizer = Adam(model.parameters(), lr=lr)
        criterion = nn.CrossEntropyLoss(ignore_index=PAD_ID)

        train_losses = []
        train_accs = []
        val_losses = []  # éªŒè¯æŸå¤±åˆ—è¡¨
        val_accs = []  # éªŒè¯å‡†ç¡®ç‡åˆ—è¡¨

        for epoch in range(n_epochs):
            model.train()
            epoch_total_loss = 0.0
            epoch_total_correct = 0.0
            epoch_total_non_pad = 0.0

            count = 1
            total = len(train_loader)

            tic = time.time()
            for i, batch in enumerate(train_loader):
                x = torch.LongTensor(batch["source"]).to(device)  # torch.Size([32, 19])
                y = torch.LongTensor(batch["target"]).to(device)  # torch.Size([32, 17])
                # ç”±äº Transformer æ˜¯åœ¨ç”¨å‰ i ä¸ª token é¢„æµ‹ç¬¬ i+1 ä¸ª token
                # è€ƒè™‘å¹¶è¡Œè®¡ç®—çš„è¯, æˆ‘ä»¬å¯ä»¥ç›´æ¥è¾“å…¥å‰ n-1 ä¸ª token, å¹¶è¡Œé¢„æµ‹å n-1 ä¸ª token
                y_output = y[:, :-1]
                y_label = y[:, 1:]
                y_hat = model(x, y_output)
                y_label_mask = y_label != PAD_ID
                preds = torch.argmax(y_hat, -1)

                correct = preds == y_label
                acc = torch.sum(y_label_mask * correct) / torch.sum(y_label_mask)

                n, seq_len = y_label.shape
                y_hat = torch.reshape(y_hat, (n * seq_len, -1))
                y_label = torch.reshape(y_label, (n * seq_len,))
                loss = criterion(y_hat, y_label)

                optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1)
                optimizer.step()

                epoch_total_loss += loss.item()
                current_correct = torch.sum(y_label_mask * correct).item()
                current_non_pad = torch.sum(y_label_mask).item()
                epoch_total_correct += current_correct
                epoch_total_non_pad += current_non_pad

                if count % print_interval == 0 or count == total:
                    toc = time.time()
                    interval = toc - tic
                    minutes = int(interval // 60)
                    seconds = int(interval % 60)
                    print(
                        f"Epoch: [{epoch+1}/{n_epochs}], Batch: [{count}/{total}], "
                        f"Loss: {loss.item()}, Acc: {acc.item()}, Time: {minutes:02d}:{seconds:02d}"
                    )
                count += 1
            avg_epoch_loss = epoch_total_loss / total
            avg_epoch_acc = epoch_total_correct / epoch_total_non_pad
            train_losses.append(avg_epoch_loss)
            train_accs.append(avg_epoch_acc)

            # è®¡ç®—éªŒè¯é›†å‡†ç¡®åº¦
            model.eval()
            val_total_loss = 0.0
            val_total_correct = 0.0
            val_total_non_pad = 0.0
            with torch.no_grad():
                for batch in val_loader:
                    x = torch.LongTensor(batch["source"]).to(device)
                    y = torch.LongTensor(batch["target"]).to(device)
                    y_output = y[:, :-1]
                    y_label = y[:, 1:]

                    # å‰å‘ä¼ æ’­
                    y_hat = model(x, y_output)
                    y_label_mask = y_label != PAD_ID
                    preds = torch.argmax(y_hat, -1)

                    # è®¡ç®—å‡†ç¡®ç‡
                    correct = preds == y_label
                    current_correct = torch.sum(y_label_mask * correct).item()
                    current_non_pad = torch.sum(y_label_mask).item()
                    val_total_correct += current_correct
                    val_total_non_pad += current_non_pad

                    # è®¡ç®—æŸå¤±
                    n, seq_len = y_label.shape
                    y_hat_flat = torch.reshape(y_hat, (n * seq_len, -1))
                    y_label_flat = torch.reshape(y_label, (n * seq_len,))
                    loss = criterion(y_hat_flat, y_label_flat)
                    val_total_loss += loss.item()

            # è®¡ç®—éªŒè¯é›†å¹³å‡æŒ‡æ ‡
            avg_val_loss = val_total_loss / len(val_loader)
            avg_val_acc = val_total_correct / val_total_non_pad
            val_losses.append(avg_val_loss)
            val_accs.append(avg_val_acc)

            # æ‰“å°è®­ç»ƒå’ŒéªŒè¯æŒ‡æ ‡
            print(
                f"Epoch: [{epoch+1}/{n_epochs}], "
                f"Avg Val loss: {avg_val_loss:.4f}, Avg Val acc: {avg_val_acc:.4f}"
            )
            print("=" * 100)

        model_path = os.path.join(output_dir, "final_model.pth")
        torch.save(model.state_dict(), model_path)
        save_plot(output_dir, train_losses, train_accs, val_losses, val_accs)
        print("Training completed.")
    ```

    è®­ç»ƒå®Œæˆå, æˆ‘ä»¬å¯ä»¥è¿›è¡Œä¸€äº›å¯è§†åŒ–çš„æ“ä½œ. å…¶ä¸­, è®­ç»ƒé›†å’ŒéªŒè¯é›†ä¸Šçš„ Loss ä»¥åŠ Accuracy çš„å˜åŒ–æ›²çº¿å¦‚ä¸‹å›¾æ‰€ç¤º: 

    ![alt text](figs/accuracy_curve.png)

    ![alt text](figs/loss_curve.png)

    ä»å›¾ä¸­å¯ä»¥å‘ç°, è™½ç„¶éšç€è®­ç»ƒ epoch çš„å¢åŠ , è®­ç»ƒé›†çš„ Loss æŒç»­ä¸‹é™, Accuracy æŒç»­ä¸Šå‡, ä½†æ˜¯åœ¨ç¬¬ 10 ä¸ª epoch ä¹‹å, éªŒè¯é›†çš„ Loss å°±å¼€å§‹ä¸Šå‡, Accuracy å¼€å§‹ä¸‹é™, è¿™è¯´æ˜æ¨¡å‹å‡ºç°äº†è¿‡æ‹Ÿåˆçš„ç°è±¡. å› æ­¤, åœ¨è¯¥è¶…å‚æ•°è®¾ç½®ä¸‹, æ¨¡å‹çš„æœ€ä½³æ•ˆæœå‡ºç°åœ¨ç¬¬ 10 ä¸ª epoch, æ­¤æ—¶éªŒè¯é›†çš„ Loss æœ€å°, Accuracy æœ€å¤§. æˆ‘ä»¬å¯ä»¥å°†è¯¥ epoch çš„æ¨¡å‹ä¿å­˜ä¸‹æ¥, ä½œä¸ºæœ€ç»ˆçš„æ¨¡å‹. 

5. æ¨¡å‹è¯„ä¼°

    åœ¨æ¨¡å‹è¯„ä¼°æ—¶, æˆ‘ä»¬ä½¿ç”¨ä¸Šæ–‡æåˆ°çš„è®­ç»ƒäº† 10 ä¸ª epoch çš„æ¨¡å‹, å¹¶ä½¿ç”¨ BLEU åˆ†æ•°æ¥è¯„ä¼°æ¨¡å‹çš„ç¿»è¯‘æ•ˆæœ. BLEU åˆ†æ•°æ˜¯ä¸€ä¸ªå¸¸ç”¨çš„æœºå™¨ç¿»è¯‘è¯„ä¼°æŒ‡æ ‡, ä¸»è¦ç”¨äºè¡¡é‡æœºå™¨ç¿»è¯‘ç»“æœä¸å‚è€ƒç¿»è¯‘ä¹‹é—´çš„ç›¸ä¼¼åº¦. BLEU åˆ†æ•°è¶Šé«˜, è¡¨ç¤ºç¿»è¯‘ç»“æœè¶Šå¥½. 
    åœ¨è¿™é‡Œ, æˆ‘ä»¬ä½¿ç”¨ `nltk.translate.bleu_score` åº“ä¸­çš„ `corpus_bleu` å‡½æ•°æ¥è®¡ç®— BLEU åˆ†æ•°. ä»£ç å®ç°å¦‚ä¸‹: 

    ```python
    from nltk.translate.bleu_score import corpus_bleu

    # ......
    elif period == "eval":
        model_path = os.path.join(output_dir, "final_model.pth")
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"Model file {model_path} not found.")
        model.load_state_dict(torch.load(model_path, weights_only=True))
        model.eval()
        with torch.no_grad():
            en_origin = []
            cn_standard = []
            cn_output = []
            for batch in tqdm(test_loader):
                x = torch.LongTensor(batch["source"]).to(device)
                y = torch.LongTensor(batch["target"]).to(device)
                batch_size = x.shape[0]
                max_len = y.shape[1]
                y_output = torch.full(
                    (batch_size, max_len), PAD_ID, dtype=torch.long, device=device
                )
                y_output[:, 0] = BOS_ID
                for cur_idx in range(1, max_len):
                    decoder_input = y_output[:, :cur_idx]
                    output = model(x, decoder_input)
                    next_tokens = torch.argmax(output[:, -1, :], dim=-1)
                    y_output[:, cur_idx] = next_tokens
                for j in range(batch_size):
                    en_origin.append(convert_to_text(x[j], int2en))
                    cn_standard.append(convert_to_text(y[j], int2cn))
                    cn_output.append(convert_to_text(y_output[j], int2cn))
        references = [[ref.split()] for ref in cn_standard]
        hypotheses = [hyp.split() for hyp in cn_output]
        bleu_score = corpus_bleu(references, hypotheses)
        print(f"BLEU Score: {bleu_score:.4f}")
    ```

    æ­¤å¤–, æˆ‘ä»¬è¿˜å¯ä»¥æ‰‹åŠ¨æ£€éªŒä¸€ä¸‹æ¨¡å‹çš„ç¿»è¯‘æ•ˆæœ, ä»£ç å®ç°å¦‚ä¸‹: 

    ```python
    print("-" * 50)
    for i in range(3):
            print(f"Original: {en_origin[i]}")
            print(f"Standard: {cn_standard[i]}")
            print(f"Translated: {cn_output[i]}")
            print("-" * 50)
    ```

    æœ€ç»ˆçš„è¾“å‡ºç»“æœå¦‚ä¸‹: 

    ```txt
    BLEU Score: 0.2322
    --------------------------------------------------
    Original: do you still want to talk to me ?
    Standard Answer: ä½  è¿˜ æƒ³ è·Ÿ æˆ‘ è°ˆ å— ï¼Ÿ
    Translated: ä½  è¿˜ æƒ³ è·Ÿ æˆ‘ è¯´ å— ï¼Ÿ
    --------------------------------------------------
    Original: i will never for ce you to marry him .
    Standard Answer: æˆ‘æ°¸è¿œ ä¸ä¼š é€¼ ä½  è·Ÿ ä»– ç»“å©š . 
    Translated: æˆ‘ ä¸ä¼š å¿˜è®° ä½  å’Œ ä»– ç»“å©š äº† . 
    --------------------------------------------------
    Original: i 'm going to go tell tom .
    Standard Answer: æˆ‘è¦ å‘Šè¯‰ æ±¤å§† . 
    Translated: æˆ‘è¦ å‘Šè¯‰ æ±¤å§† . 
    --------------------------------------------------
    ```

    BLEU åˆ†æ•°ä¸º 0.2322, è¯´æ˜æ¨¡å‹çš„ç¿»è¯‘æ•ˆæœè¿˜ä¸é”™. åŒæ—¶, æˆ‘ä»¬å¯ä»¥çœ‹åˆ°, æ‰‹åŠ¨è¾“å‡ºçš„å‡ ä¸ªç¤ºä¾‹ä¸­, æ¨¡å‹çš„ç¿»è¯‘æ•ˆæœä¹Ÿè¾ƒä¸ºç†æƒ³. è™½ç„¶æœ‰äº›åœ°æ–¹ç¿»è¯‘å¾—ä¸æ˜¯å¾ˆæ­£ç¡®, ä¾‹å¦‚å°† "force" ç¿»è¯‘æˆäº† "å¿˜è®°", ä½†æ•´ä½“ä¸Šè¿˜æ˜¯ç¬¦åˆé€»è¾‘ä¸”è¾ƒä¸ºå‡†ç¡®çš„. 

## ğŸ“º æ¼”ç¤ºè§†é¢‘

[æ¼”ç¤ºè§†é¢‘](https://box.nju.edu.cn/f/e0f8854b0b0a47fbac4c/) ä¸­åŒ…å«äº†æ¨¡å‹çš„è®­ç»ƒå’Œè¯„ä¼°è¿‡ç¨‹.

## ğŸ“œ å‚è€ƒèµ„æ–™

[1] [Attention Is All You Need. NeurIPS 2017](https://arxiv.org/abs/1706.03762)

[2] [Yi-Fan Zhou's Blog](https://zhuanlan.zhihu.com/p/581334630)

[3] [æœºå™¨ç¿»è¯‘è¯„ä»·æŒ‡æ ‡BLEUä»‹ç»](https://blog.csdn.net/g11d111/article/details/100103208)

[4] [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)

[5] [Learning Deep Transformer Models for Machine Translation. ACL 2019](https://arxiv.org/abs/1906.01787)

[6] [ã€Šç¥ç»ç½‘ç»œä¸æ·±åº¦å­¦ä¹ ã€‹ é‚±é”¡é¹](https://nndl.github.io/)